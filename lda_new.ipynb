{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "enron_df = pd.read_pickle('enron_students.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "enron_df['To'] = enron_df['To'].fillna('')\n",
    "enron_df['From'] = enron_df['From'].fillna('')\n",
    "enron_df['X-From'] = enron_df['X-From'].fillna('')\n",
    "enron_df['X-To'] = enron_df['X-To'].fillna('')\n",
    "enron_df['X-cc'] = enron_df['X-cc'].fillna('')\n",
    "enron_df['X-bcc'] = enron_df['X-bcc'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed attachments\n",
      "parsed contacts\n",
      "removed foward text\n",
      "filled nan values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\private\\Cyber and Machine learning\\Assignments\\cyber-ai\\preprocessing.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  enron_df['email_text'].fillna(' ', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed email headers\n",
      "removed small words\n",
      "lemmatized\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "\n",
    "enron_df['email_text'] = enron_df['Subject'] + ' ' + enron_df['email_body']\n",
    "preprocessing.data_cleaning(enron_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enron_df.to_pickle('enron_students_cleaned.pkl')\n",
    "import pandas as pd\n",
    "\n",
    "enron_df = pd.read_pickle(r'enron_students_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "def build_lda_model(df, num_topics=3, threshold=0.3):\n",
    "\n",
    "    # Extract and tokenize the email body\n",
    "    df['tokens'] = df['email_text'].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "    print(\"Number of unique tokens before filtering:\", len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    print(\"Number of unique tokens after filtering:\", len(dictionary))\n",
    "\n",
    "    # Create bag-of-words representation of the documents\n",
    "    df['bow'] = df['tokens'].apply(dictionary.doc2bow)\n",
    "    # Filter out documents that result in no words after processing\n",
    "    bow_corpus = df[df['bow'].map(len) > 0]\n",
    "\n",
    "    # Check if there are any valid documents left\n",
    "    if bow_corpus.empty:\n",
    "        raise ValueError(\"No valid documents to process. All documents resulted in empty BOW representations.\")\n",
    "\n",
    "    # Build an LDA model\n",
    "    lda_model = LdaMulticore(corpus=list(bow_corpus['bow']),\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics,\n",
    "                             chunksize=1000,\n",
    "                             passes=8,\n",
    "                             per_word_topics=True,\n",
    "                             workers=8)\n",
    "\n",
    "    return lda_model, bow_corpus, dictionary\n",
    "\n",
    "\n",
    "def get_topic_assignments(lda_model: LdaMulticore, bow_corpus, threshold=0.3):\n",
    "    \n",
    "    # Assign documents to topics\n",
    "    topic_assignments = []\n",
    "    for bow in bow_corpus:\n",
    "        topic_probs = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "        # Sort topics by probability\n",
    "        topic_probs = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "        # Assign \"None of these topics\" if the highest probability is below the threshold\n",
    "        if topic_probs[0][1] < threshold:\n",
    "            topic_assignments.append(\"None of these topics\")\n",
    "        else:\n",
    "            topic_assignments.append(f\"Topic {topic_probs[0][0]}\")\n",
    "\n",
    "    return topic_assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 % of the enron rows\n",
    "enron_small = enron_df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 30 topics.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lda_model, bow_corpus, dictionary = build_lda_model(enron_df, num_topics=30, threshold=0.3)\n",
    "    print(\"Model built successfully with {} topics.\".format(lda_model.num_topics))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 20 topics.\n",
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 25 topics.\n",
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 30 topics.\n",
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 40 topics.\n"
     ]
    }
   ],
   "source": [
    "# def run_lda_multiple_topics(df, topic_numbers):\n",
    "#     lda_model_res = {}\n",
    "#     dictionary_res = {}\n",
    "#     corpus_res = {}\n",
    "#     for num_topics in topic_numbers:\n",
    "#         try:\n",
    "#             lda_model, bow_corpus, dictionary = build_lda_model(df, num_topics=num_topics, threshold=0.3)\n",
    "#             print(\"Model built successfully with {} topics.\".format(lda_model.num_topics))\n",
    "#             # Optionally, calculate and print model coherence, perplexity, etc. here\n",
    "#             lda_model_res[num_topics] = lda_model  # Store the model, or just store metrics like coherence\n",
    "#             dictionary_res[num_topics] = dictionary\n",
    "#             corpus_res[num_topics] = bow_corpus\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Failed to build model with {num_topics} topics: {e}\")\n",
    "#     return lda_model_res, dictionary_res, corpus_res\n",
    "\n",
    "# # Define the range of topics you want to test\n",
    "# topic_numbers = [20, 25, 30, 40]\n",
    "\n",
    "# # Assuming enron_df is your DataFrame\n",
    "# lda_model_res, dictionary_res, corpus_res = run_lda_multiple_topics(enron_df, topic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens before filtering: 271829\n",
      "Number of unique tokens after filtering: 100000\n",
      "Model built successfully with 50 topics.\n"
     ]
    }
   ],
   "source": [
    "# topic_numbers_large = [50]\n",
    "\n",
    "# lda_model_res_large, dictionary_res_large, corpus_res_large = run_lda_multiple_topics(enron_df, topic_numbers_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Efraim Yosofov\\AppData\\Local\\Temp\\ipykernel_9412\\4117866169.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bow_corpus['Topic'] = [get_most_probable_topic(doc, lda_model, True) for doc in bow_corpus['bow']]\n"
     ]
    }
   ],
   "source": [
    "def get_most_probable_topic(doc, model, get_destribution=False):\n",
    "    if not doc:\n",
    "        return None  # Handle empty documents\n",
    "    topic_probs = model.get_document_topics(doc, minimum_probability=0)\n",
    "    if not topic_probs:\n",
    "        return None  # Handle cases with no significant topic probability\n",
    "    \n",
    "    if get_destribution:\n",
    "        return topic_probs\n",
    "    \n",
    "    return max(topic_probs, key=lambda x: x[1])[0]  # Return only the topic index\n",
    "\n",
    "# Assume lda_model and valid_df are from your previous output\n",
    "bow_corpus['Topic'] = [get_most_probable_topic(doc, lda_model, True) for doc in bow_corpus['bow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Efraim Yosofov\\AppData\\Local\\Temp\\ipykernel_28696\\17654098.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus_res[40]['Topic'] = [get_most_probable_topic(doc, lda_model_res[40], True) for doc in corpus_res[40]['bow']]\n"
     ]
    }
   ],
   "source": [
    "# corpus_res_large[50]['Topic'] = [get_most_probable_topic(doc, lda_model_res_large[50], True) for doc in corpus_res_large[50]['bow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Visualize the topics\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m vis \u001b[38;5;241m=\u001b[39m \u001b[43mgensimvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbow_corpus\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(vis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyLDAvis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyLDAvis\\gensim_models.py:17\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gensim\u001b[38;5;241m.\u001b[39mmatutils\u001b[38;5;241m.\u001b[39mismatrix(corpus):\n\u001b[1;32m---> 17\u001b[0m     corpus_csc \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus2csc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_terms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     corpus_csc \u001b[38;5;241m=\u001b[39m corpus\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\matutils.py:152\u001b[0m, in \u001b[0;36mcorpus2csc\u001b[1;34m(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)\u001b[0m\n\u001b[0;32m    149\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# zip(*doc) transforms doc to (token_indices, token_counts]\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m doc_indices, doc_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n\u001b[0;32m    153\u001b[0m indices\u001b[38;5;241m.\u001b[39mextend(doc_indices)\n\u001b[0;32m    154\u001b[0m data\u001b[38;5;241m.\u001b[39mextend(doc_data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "vis = gensimvis.prepare(lda_model, list(bow_corpus['bow']), dictionary)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyLDAvis\\_display.py:262\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(data, ip, port, n_retries, local, open_browser, http_server, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md3_url\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mD3_URL\n\u001b[0;32m    260\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mldavis_css_url\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mLDAVIS_CSS_URL\n\u001b[0;32m    261\u001b[0m     files \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/LDAvis.js\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/javascript\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mopen\u001b[39m(urls\u001b[38;5;241m.\u001b[39mLDAVIS_LOCAL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()],\n\u001b[1;32m--> 262\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/LDAvis.css\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/css\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLDAVIS_CSS_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()],\n\u001b[0;32m    263\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/d3.js\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/javascript\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mopen\u001b[39m(urls\u001b[38;5;241m.\u001b[39mD3_URL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()]}\n\u001b[0;32m    264\u001b[0m html \u001b[38;5;241m=\u001b[39m prepared_data_to_html(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m serve(html, ip\u001b[38;5;241m=\u001b[39mip, port\u001b[38;5;241m=\u001b[39mport, n_retries\u001b[38;5;241m=\u001b[39mn_retries, files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    266\u001b[0m       open_browser\u001b[38;5;241m=\u001b[39mopen_browser, http_server\u001b[38;5;241m=\u001b[39mhttp_server)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css'"
     ]
    }
   ],
   "source": [
    "# pyLDAvis.show(vis, local=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'From', 'To', 'X-To', 'X-From', 'X-cc', 'X-bcc', 'Subject',\n",
       "       'email_body', 'verdict', 'violated_rules', 'email_text', 'is_list',\n",
       "       'tokens', 'bow'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best topic for each label\n",
    "# label_1_1 = enron_small[enron_small['label']]\n",
    "enron_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -14.035978990452321\n",
      "Coherence Score:  0.599971752458084\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import CoherenceModel\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# # Calculate perplexity for LDA model\n",
    "# def get_score():\n",
    "#     perplexity = lda_model_res[40].log_perplexity(list(corpus_res[40]['bow']))\n",
    "#     print(\"Perplexity: \", perplexity)\n",
    "\n",
    "#     coherence_model_lda = CoherenceModel(model=lda_model_res[40], texts=corpus_res[40]['tokens'], dictionary=dictionary_res[40], coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "#     print('Coherence Score: ', coherence_lda)\n",
    "\n",
    "# print(get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i want to get the most probable topic for each email\n",
    "# print(corpus_res[40]['Topic'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_res[40].to_pickle('enron_students_lda_40_corpus.pkl')\n",
    "# dictionary_res[40].to_pickle('enron_students_lda_40_dict.pkl')\n",
    "# lda_model_res[40].to_pickle('enron_students_lda_40_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_topic_for_label(model, dictionary, corpus, label):\n",
    "    # get the dataset with labels. the corpus already have the topic column\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
