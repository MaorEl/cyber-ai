{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.157334Z",
     "start_time": "2024-05-11T15:01:01.748106Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy \n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "import dataclasses\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "USE_CACHE_DICTS = True # set to False to recompute all dictionaries\n",
    "\n",
    "# # Read dictionary pkl file\n",
    "# with open('email_to_departments.pkl', 'rb') as fp:\n",
    "#     email_to_departments = pickle.load(fp)\n",
    "#     print('email_to_departments dictionary loaded from pkl file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.767303Z",
     "start_time": "2024-05-11T15:01:02.158816Z"
    }
   },
   "outputs": [],
   "source": [
    "enron_df = pd.read_pickle('enron_students.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.904957Z",
     "start_time": "2024-05-11T15:01:02.768028Z"
    }
   },
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "enron_df['To'] = enron_df['To'].fillna('')\n",
    "enron_df['From'] = enron_df['From'].fillna('')\n",
    "enron_df['X-From'] = enron_df['X-From'].fillna('')\n",
    "enron_df['X-To'] = enron_df['X-To'].fillna('')\n",
    "enron_df['X-cc'] = enron_df['X-cc'].fillna('')\n",
    "enron_df['X-bcc'] = enron_df['X-bcc'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.909514Z",
     "start_time": "2024-05-11T15:01:02.906086Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_emails(text):\n",
    "    if pd.isna(text) or text is None or text == '':\n",
    "        return []\n",
    "    # Regex to match email addresses\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "def extract_single_email_from_text(text):\n",
    "    emails = extract_emails(text)\n",
    "    if (len(emails) > 1):\n",
    "        print(f\"Multiple emails found in: {text}\")\n",
    "    elif len(emails) == 1:\n",
    "        return emails[0]\n",
    "    print(f\"Could not extract email from: {text}\")\n",
    "    return None\n",
    "\n",
    "def extract_sent_to_emails(text):\n",
    "    emails = extract_emails(text)\n",
    "    if len(emails) > 0:\n",
    "        return emails\n",
    "    return []\n",
    "\n",
    "def is_between_ect_and_ees(email_to_depart_dict, row, is_including_cc_bcc=False, additional_check=True):\n",
    "    sender_email = extract_single_email_from_text(row['From'])\n",
    "    if sender_email in email_to_depart_dict:\n",
    "        sender_department = email_to_depart_dict[sender_email]\n",
    "        if sender_department != 'ECT' and sender_department != 'EES':\n",
    "            return False\n",
    "        else:\n",
    "            if is_including_cc_bcc:\n",
    "                reciever_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To'])\n",
    "            else:\n",
    "                reciever_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To']) + extract_sent_to_emails(row['X-cc']) + extract_sent_to_emails(row['X-bcc'])\n",
    "            \n",
    "            for email in reciever_emails:\n",
    "                if email in email_to_depart_dict:\n",
    "                    reciever_department = email_to_depart_dict[email]\n",
    "\n",
    "                    if sender_department == 'ECT' and reciever_department == 'EES':\n",
    "                        return True\n",
    "                    elif sender_department == 'EES' and reciever_department == 'ECT':\n",
    "                        return True\n",
    "                    \n",
    "            if additional_check:\n",
    "                if 'ECT' in row['X-To']:\n",
    "                    if 'EES' in row['X-From']:\n",
    "                        return True\n",
    "                if 'EES' in row['X-To']:\n",
    "                    if 'ECT' in row['X-From']:\n",
    "                        return True\n",
    "      \n",
    "            return False\n",
    "    \n",
    "    else:\n",
    "        if additional_check:\n",
    "            if 'ECT' in row['X-To']:\n",
    "                if 'EES' in row['X-From']:\n",
    "                    return True\n",
    "            if 'EES' in row['X-To']:\n",
    "                if 'ECT' in row['X-From']:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.918666Z",
     "start_time": "2024-05-11T15:01:02.910322Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_dict():\n",
    "    email_to_departments = {}\n",
    "    email_to_departments.update({\n",
    "            'smith.day@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stuart.zisman@enron.com' : 'ECT'\n",
    "            ,'melissa.murphy@enron.com' : 'ECT'\n",
    "            ,'thresa.allen@enron.com' : 'ECT'\n",
    "            ,'leslie.reeves@enron.com' : 'ECT'\n",
    "            ,'stacey.white@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'brad.nebergall@enron.com': 'ECT'\n",
    "            ,'colleen.koenig@enron.com': 'EES'\n",
    "            ,'john.kinser@enron.com': 'ECT'\n",
    "            ,'jeff.merola@enron.com': 'EES'\n",
    "            })\n",
    "    \n",
    "    email_to_departments.update({'eric.bass@enron.com': 'ECT'})\n",
    "\n",
    "    multiple_count = 0\n",
    "    for index, row in enron_df.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        from_emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        to_emails = set([mail.lower() for mail in (extract_emails(row['To']) + extract_emails(row['X-To']))])\n",
    "        if len(from_emails) == 0:\n",
    "            continue\n",
    "        if len(from_emails) > 1:\n",
    "            from_emails = [mail for mail in from_emails if not mail.startswith('imceanotes')]\n",
    "            if len(from_emails) > 1 or len(from_emails) == 0:\n",
    "                multiple_count += 1\n",
    "\n",
    "        for email in from_emails:\n",
    "            email = email.lower()\n",
    "            if \"enron.com\" not in email and email not in email_to_departments:\n",
    "                email_to_departments.update({email: 'NA'})\n",
    "            else:\n",
    "                if email in email_to_departments and email_to_departments[email] != 'Other':\n",
    "                    continue\n",
    "                elif 'EES' in row['X-From']:\n",
    "                    email_to_departments.update({email: 'EES'})\n",
    "                    if len(to_emails) == 1:\n",
    "                        email_to_departments.update({list(to_emails)[0]: 'ECT'})\n",
    "                elif 'ECT' in row['X-From']:\n",
    "                    email_to_departments.update({email: 'ECT'})\n",
    "                    if len(to_emails) == 1:\n",
    "                        email_to_departments.update({list(to_emails)[0]: 'EES'})\n",
    "                else:\n",
    "                    email_to_departments.update({email: 'Other'})\n",
    "\n",
    "        # new addition - need to check\n",
    "        if len(to_emails) == 1:\n",
    "            if 'ECT' in row['X-To']:\n",
    "                email_to_departments.update({list(to_emails)[0]: 'ECT'})\n",
    "            elif 'EES' in row['X-To']:\n",
    "                email_to_departments.update({list(to_emails)[0]: 'EES'})\n",
    "\n",
    "    email_to_departments.update({\n",
    "            'smith.day@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stuart.zisman@enron.com' : 'ECT'\n",
    "            ,'melissa.murphy@enron.com' : 'ECT'\n",
    "            ,'thresa.allen@enron.com' : 'ECT'\n",
    "            ,'leslie.reeves@enron.com' : 'ECT'\n",
    "            ,'stacey.white@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'brad.nebergall@enron.com': 'ECT'\n",
    "            ,'colleen.koenig@enron.com': 'EES'\n",
    "            ,'john.kinser@enron.com': 'ECT'\n",
    "            ,'jeff.merola@enron.com\t': 'EES'\n",
    "            ,'scott.mills@enron.com': 'ECT'\n",
    "            ,'marilyn.colbert@enron.com': 'ECT'\n",
    "            ,'molly.harris@enron.com': 'ECT'\n",
    "            ,'joseph.wagner@enron.com': 'ECT'\n",
    "            ,'stuart.zisman@enron.com,' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stephanie.gardner@enron.com' : 'EES'\n",
    "\n",
    "            })\n",
    "    \n",
    "    return email_to_departments\n",
    "\n",
    "if USE_CACHE_DICTS:\n",
    "    email_to_department_dict = pickle.load(open('email_to_departments.pkl', 'rb'))\n",
    "else:\n",
    "    email_to_department_dict = build_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:02.920964Z",
     "start_time": "2024-05-11T15:01:02.919277Z"
    }
   },
   "outputs": [],
   "source": [
    "len(email_to_department_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.213634Z",
     "start_time": "2024-05-11T15:01:02.921457Z"
    }
   },
   "outputs": [],
   "source": [
    "enron_df['is_between_ect_and_ees'] = enron_df.apply(lambda row: is_between_ect_and_ees(email_to_department_dict, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.215880Z",
     "start_time": "2024-05-11T15:01:06.214310Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'cheryl.dudley@enron.com' in email_to_department_dict:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.484514Z",
     "start_time": "2024-05-11T15:01:06.216431Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all spaces \n",
    "enron_df['violated_rules'] = enron_df['violated_rules'].apply(lambda x: x.replace(' ', ''))\n",
    "\n",
    "# apply split ',' on violated rules\n",
    "enron_df['violated_rules'] = enron_df['violated_rules'].apply(lambda x: x.split(',')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.636397Z",
     "start_time": "2024-05-11T15:01:06.486491Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "enron_df_not_catched_1_1 = enron_df[(enron_df['is_between_ect_and_ees'] == False) & (enron_df['violated_rules'].apply(lambda x: '1.1' in x ) & (enron_df['X-To'].str.contains('ECT')))]\n",
    "enron_df_not_catched_1_1.head(10)\n",
    "# len(enron_df_not_catched_1_1)\n",
    "\n",
    "# print(is_between_ect_and_ees(test, enron_df_not_catched_1_1.iloc[0], False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.640018Z",
     "start_time": "2024-05-11T15:01:06.636984Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_dict(enron_df_not_catched_1_1, email_to_depart):\n",
    "    # iterate over the rows of the dataframe enron_df_not_catched_1_1\n",
    "    updated = 0\n",
    "    for index, row in enron_df_not_catched_1_1.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        sent_to_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To'])\n",
    "        if len(emails) == 0:\n",
    "            continue\n",
    "        if len(emails) > 1:\n",
    "            emails = [mail for mail in emails if not mail.startswith('imceanotes')]\n",
    "            if len(emails) > 1 or len(emails) == 0:\n",
    "                # print(f\"From: {row['From']} X-From: {row['X-From']} has multiple emails: {emails}\")\n",
    "                continue\n",
    "        for email in emails:\n",
    "            email = email.lower()\n",
    "            # if from email is in dictionary, get the department\n",
    "            if email in email_to_depart:\n",
    "                depart = email_to_depart[email]\n",
    "                if depart == 'ECT':\n",
    "                    if len(sent_to_emails) == 1:\n",
    "                        email_to_depart[sent_to_emails[0]] = 'EES'\n",
    "                        updated += 1\n",
    "                elif depart == 'EES':\n",
    "                    if len(sent_to_emails) == 1:\n",
    "                        email_to_depart[sent_to_emails[0]] = 'ECT'\n",
    "                        updated += 1\n",
    "\n",
    "    print(f\"Updated {updated} emails\")\n",
    "    \n",
    "\n",
    "\n",
    "update_dict(enron_df_not_catched_1_1, email_to_department_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HERE WE START USER TO LOCATION *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.644746Z",
     "start_time": "2024-05-11T15:01:06.640649Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_email_to_location_dict(df):\n",
    "    email_to_location = {}\n",
    "    for index, row in enron_df.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        from_emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        if '/OU=EU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'EU'})\n",
    "        elif '/HOU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        elif '/OU=NA/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "\n",
    "    # iterate over the dataframe to get \n",
    "    violated_rule_1_2 = df[df['violated_rules'].apply(lambda x: '1.2' in x)]\n",
    "\n",
    "    for index, row in violated_rule_1_2.iterrows():\n",
    "        from_emails = extract_emails(row['X-From'])\n",
    "        from_emails = list(set([mail.lower() for mail in (extract_emails(row['From']) + from_emails)]))\n",
    "        if '/OU=EU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'EU'})\n",
    "        elif '/HOU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        elif '/OU=NA/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        \n",
    "        sent_to_email = extract_emails(row['X-To'])\n",
    "        if len(sent_to_email) == 1:\n",
    "            if from_emails[0] not in email_to_location:\n",
    "                continue\n",
    "            if email_to_location[from_emails[0]] == 'EU':\n",
    "                email_to_location.update({sent_to_email[0]: 'NA'})\n",
    "            elif email_to_location[from_emails[0]] == 'NA':\n",
    "                email_to_location.update({sent_to_email[0]: 'EU'})\n",
    "            \n",
    "    return email_to_location\n",
    "\n",
    "if USE_CACHE_DICTS:\n",
    "    email_to_location = pickle.load(open('email_to_location.pkl', 'rb'))\n",
    "else:\n",
    "    email_to_location = get_email_to_location_dict(enron_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:06.647356Z",
     "start_time": "2024-05-11T15:01:06.645336Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_between_EU_and_NA(row, email_to_location):\n",
    "    from_email = extract_emails(row['From'])\n",
    "    from_email = extract_emails(row['X-From']) + from_email\n",
    "    from_email = list(set([mail.lower() for mail in from_email]))\n",
    "    to_email = extract_emails(row['To'])\n",
    "    to_email = extract_emails(row['X-To']) + to_email\n",
    "    to_email = list(set([mail.lower() for mail in to_email]))\n",
    "    if len(from_email) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "    if from_email[0] in email_to_location:\n",
    "        if email_to_location[from_email[0]] == 'EU':\n",
    "            for email in to_email:\n",
    "                if email in email_to_location and email_to_location[email] == 'NA':\n",
    "                    return True\n",
    "        if email_to_location[from_email[0]] == 'NA':\n",
    "            for email in to_email:\n",
    "                if email in email_to_location and email_to_location[email] == 'EU':\n",
    "                    return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:11.842986Z",
     "start_time": "2024-05-11T15:01:06.647925Z"
    }
   },
   "outputs": [],
   "source": [
    "enron_df['is_EU_To_NA'] = enron_df.apply(lambda row: is_between_EU_and_NA(row, email_to_location), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:11.889621Z",
     "start_time": "2024-05-11T15:01:11.843583Z"
    }
   },
   "outputs": [],
   "source": [
    "# not_working = enron_df[(enron_df['is_EU_To_NA'] == False) & (enron_df['violated_rules'].apply(lambda x: '1.2' in x))]\n",
    "# not_working.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:11.891803Z",
     "start_time": "2024-05-11T15:01:11.890299Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(email_to_location))\n",
    "# enron_df.head(10)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:11.895526Z",
     "start_time": "2024-05-11T15:01:11.892466Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_c_suit_dict():\n",
    "    emails = set({\n",
    "        'kenneth.lay@enron.com',\n",
    "        'ken.skilling@enron.com',\n",
    "        'rbowers@nyiso.com',\n",
    "        'michael.brown@enron.com',\n",
    "        'john.sherriff@enron.com',\n",
    "        'jeffrey.sherrick@enron.com',\n",
    "        'a..shankman@enron.com',\n",
    "        'ken.rice@enron.com',\n",
    "        'greg.piper@enron.com',\n",
    "        'mark.metts@enron.com',\n",
    "        'coo.jeff@enron.com',\n",
    "        'rebecca.mcdonald@enron.com',\n",
    "        'danny.mccarty@enron.com',\n",
    "        'dan.leff@enron.com',\n",
    "        'john.lavorato@enron.com',\n",
    "        'mark.koenig@enron.com',\n",
    "        'louise.kitchen@enron.com',\n",
    "        'stanley.horton@enron.com',\n",
    "        '40enron@enron.com', # for some reason is tagged for stanley horton\n",
    "        'ben.glisan@enron.com',\n",
    "        'mark.frevert@enron.com',\n",
    "        'andrew.fastow@enron.com',\n",
    "        'jr..legal@enron.com',\n",
    "        'derrick@enron.com',\n",
    "        'david.delainey@enron.com',\n",
    "        'richard.causey@enron.com',\n",
    "        'michael.brown@enron.com',\n",
    "        'raymond.bowen@enron.com',\n",
    "    })\n",
    "\n",
    "    violated_rule_1_3 = enron_df[enron_df['violated_rules'].apply(lambda x: '2.2' in x)]\n",
    "\n",
    "    for index, row in violated_rule_1_3.iterrows():\n",
    "        from_emails = extract_emails(row['X-From'])\n",
    "        from_emails = list(set([mail.lower() for mail in (extract_emails(row['From']) + from_emails)]))\n",
    "        to_email = extract_emails(row['To'])\n",
    "        to_email = extract_emails(row['X-To']) + to_email\n",
    "        to_email = list(set([mail.lower() for mail in to_email]))\n",
    "\n",
    "        if len(from_emails) == 0:\n",
    "            continue\n",
    "        for email in from_emails:\n",
    "            emails.add(email)\n",
    "            # print(f\"Email: {email} is in C-Suit\")\n",
    "\n",
    "        # for email in to_email:\n",
    "        #     emails.add(email)\n",
    "        #     print(f\"Email: {email} is in C-Suit\")\n",
    "\n",
    "    return emails\n",
    "\n",
    "if USE_CACHE_DICTS:\n",
    "    c_suit_emails = pickle.load(open('c_suit_emails.pkl', 'rb'))\n",
    "else:\n",
    "    c_suit_emails = get_c_suit_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:11.897868Z",
     "start_time": "2024-05-11T15:01:11.896027Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_in_c_suit(row, c_suit_emails):\n",
    "    # Extract and clean email addresses from 'From' fields\n",
    "    from_emails = extract_emails(row['X-From']) + extract_emails(row['From'])\n",
    "    from_emails = list(set(email.lower() for email in from_emails))\n",
    "\n",
    "    # Extract and clean email addresses from 'To' fields\n",
    "    to_email = extract_emails(row['X-To']) + extract_emails(row['To'])\n",
    "    to_email = list(set(email.lower() for email in to_email))\n",
    "\n",
    "    # Check if both sender and recipient are in the c-suite email list\n",
    "    return any(email in c_suit_emails for email in from_emails) and any(email in c_suit_emails for email in to_email)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.192452Z",
     "start_time": "2024-05-11T15:01:11.898421Z"
    }
   },
   "outputs": [],
   "source": [
    "enron_df['is_c_suit'] = enron_df.apply(lambda row: is_in_c_suit(row, c_suit_emails), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.194554Z",
     "start_time": "2024-05-11T15:01:17.193217Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.196622Z",
     "start_time": "2024-05-11T15:01:17.195045Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_from_to_emails(row):\n",
    "    from_emails = extract_emails(row['X-From']) + extract_emails(row['From'])\n",
    "    from_emails = list(set(email.lower() for email in from_emails))\n",
    "\n",
    "    # Extract and clean email addresses from 'To' fields\n",
    "    to_email = extract_emails(row['X-To']) + extract_emails(row['To'])\n",
    "    to_email = list(set(email.lower() for email in to_email))\n",
    "\n",
    "    return from_emails, to_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.199726Z",
     "start_time": "2024-05-11T15:01:17.197225Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_leaving_corporate(row):\n",
    "    from_emails, to_email = get_from_to_emails(row)\n",
    "    if len(from_emails) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "    \n",
    "    for email in to_email:\n",
    "        if 'enron.com' not in email:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_leaving_ect(row, email_to_depart):\n",
    "    from_emails, to_email = get_from_to_emails(row)\n",
    "    if len(from_emails) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "\n",
    "    for email in from_emails:\n",
    "        if email in email_to_depart and email_to_depart[email] == 'ECT':\n",
    "            for t_email in to_email:\n",
    "                if t_email in email_to_depart and email_to_depart[t_email] != 'ECT':\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.201948Z",
     "start_time": "2024-05-11T15:01:17.200249Z"
    }
   },
   "outputs": [],
   "source": [
    "# save all dictionaries as pickle\n",
    "import pickle\n",
    "global USE_CACHE_DICTS\n",
    "if not USE_CACHE_DICTS:\n",
    "    pickle.dump(email_to_location, open('email_to_location.pkl', 'wb'))\n",
    "    pickle.dump(email_to_department_dict, open('email_to_departments.pkl', 'wb'))\n",
    "    pickle.dump(c_suit_emails, open('c_suit_emails.pkl', 'wb'))\n",
    "    print('All dictionaries saved as pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.204290Z",
     "start_time": "2024-05-11T15:01:17.202485Z"
    }
   },
   "outputs": [],
   "source": [
    "class StaticAnalyzer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_1(row):\n",
    "        return is_between_ect_and_ees(email_to_department_dict, row, is_including_cc_bcc=False, additional_check=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_2(row):\n",
    "        return is_between_EU_and_NA(row, email_to_location)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_3(row):\n",
    "        return is_leaving_corporate(row)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_1(row):\n",
    "        return is_leaving_ect(row, email_to_department_dict)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_2(row):\n",
    "        return is_in_c_suit(row, c_suit_emails)\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_3(row):\n",
    "        return is_leaving_corporate(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.207678Z",
     "start_time": "2024-05-11T15:01:17.204776Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class ContentAnalysisResult:\n",
    "    quids: []\n",
    "    piis: []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.quids = []\n",
    "        self.piis = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'(Sensitive: {len(self.piis) > 0 or len(self.quids) > 0} | Quids: {self.quids} | PII: {self.piis})'\n",
    "    \n",
    "    def is_sensitive(self, rule_id):\n",
    "        if rule_id == '2.3':\n",
    "            return len(self.piis) > 0 or len(self.quids) >= 2\n",
    "        else:\n",
    "            return len(self.piis) > 0 or len(self.quids) > 0\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class TopicAnalysisResult:\n",
    "    is_legal: bool = False\n",
    "    is_business: bool = False\n",
    "    is_finance: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'(Legal: {self.is_legal} | Business: {self.is_business} | Finance: {self.is_finance})'\n",
    "    \n",
    "@dataclasses.dataclass()\n",
    "class EnforcerResult:\n",
    "    violated_rules: []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.violated_rules = []\n",
    "\n",
    "    def is_allowed(self)-> bool:\n",
    "        return len(self.violated_rules) == 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Violated Rules: {self.violated_rules}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.216617Z",
     "start_time": "2024-05-11T15:01:17.208251Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_all_ssn(text):\n",
    "    return re.findall(r'\\d{3}-\\d{2}-\\d{4}', text)\n",
    "\n",
    "def find_all_credit_cards(text):\n",
    "    return re.findall(r'\\d{4}-\\d{4}-\\d{4}-\\d{4}', text) \n",
    "\n",
    "def find_all_phone_numbers(text):\n",
    "    return re.findall(r'\\(?\\d{3}\\)?\\s*-\\s*\\d{3}\\s*-\\s*\\d{4}', text) \n",
    "\n",
    "def find_sensitive_words(text):\n",
    "    return re.findall(r'password|attach|confidential', text.lower())\n",
    "\n",
    "\n",
    "class PII:\n",
    "    def __init__(self, entity_type, score, text):\n",
    "        self.entity_type = entity_type\n",
    "        self.score = score\n",
    "        self.text = text\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'(Entity: {self.entity_type} | Score: {self.score} | Text: {self.text})'\n",
    "        \n",
    "class ContentAnalyzer:\n",
    "    \n",
    "    MAX_DOC_SIZE_FOR_SPACY = 2000\n",
    "    QUIDS = ['ORG', 'GPE', 'LOW', 'FAC', 'LOC']\n",
    "    SENSITIVE = ['MONEY', 'PERCENT', 'NORP', 'PRODUCT', 'EVENT']\n",
    "    POTENTIALLY_SENSITIVE = ['DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL', 'PERSON']\n",
    "    \n",
    "    def __init__(self, software='spacy'):      \n",
    "        self.software = software\n",
    "        if software == 'spacy':\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            self.nlp.max_length = 1500000\n",
    "        elif software == 'presidio':\n",
    "            self.analyzer = AnalyzerEngine()\n",
    "        else:\n",
    "            raise Exception(f'Software {software} is not supported')\n",
    "\n",
    "        \n",
    "    def _get_entities(self, document):\n",
    "        try:\n",
    "            doc_len = len(document)\n",
    "            if doc_len > self.MAX_DOC_SIZE_FOR_SPACY:\n",
    "                document = document[:self.MAX_DOC_SIZE_FOR_SPACY]\n",
    "            doc = self.nlp(document)\n",
    "            return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "        except ValueError as e :\n",
    "            print(f'Error in document. Error: {e}')\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def _predict_spacy_verdict(self, row) -> ContentAnalysisResult:\n",
    "        result = ContentAnalysisResult()\n",
    "        count_persons = 0\n",
    "        sensitive_words = find_sensitive_words(row['email_text'])\n",
    "        if len(sensitive_words) > 0:\n",
    "            for word in sensitive_words:\n",
    "                result.piis.append(word)\n",
    "        if row['quids'] and len(row['quids']) > 0: \n",
    "            result.quids = row['quids']\n",
    "        elif row['sensitive'] and len(row['sensitive']) > 0:\n",
    "            result.piis.append(row['sensitive'])\n",
    "        elif row['potentially_sensitive'] and len(row['potentially_sensitive']) > 0:\n",
    "            for ent in row['potentially_sensitive']:\n",
    "                if ent[1] == 'DATE':\n",
    "                    continue\n",
    "                elif ent[1] == 'TIME':\n",
    "                    continue\n",
    "                elif ent[1] == 'QUANTITY':\n",
    "                    continue\n",
    "                elif ent[1] == 'ORDINAL':\n",
    "                    continue\n",
    "                elif ent[1] == 'CARDINAL':\n",
    "                    phone_numbers = find_all_phone_numbers(ent[0])\n",
    "                    credit_cards = find_all_credit_cards(ent[0])\n",
    "                    ssns = find_all_ssn(ent[0])\n",
    "                    for phone_number in phone_numbers:\n",
    "                        result.piis.append(phone_number)\n",
    "                    for credit_card in credit_cards:\n",
    "                        result.piis.append(credit_card)\n",
    "                    for ssn in ssns:\n",
    "                        result.piis.append(ssn)\n",
    "                \n",
    "                #todo maor - think about this, do we want to enter 2 qids in that case? let's see\n",
    "                # elif ent[1] == 'PERSON':\n",
    "                #     count_persons += 1\n",
    "                #     if count_persons > 2:\n",
    "                #         return 'Sensitive'\n",
    "                #     continue\n",
    "        return result\n",
    "    \n",
    "    def analyze(self, row: pd.DataFrame) -> ContentAnalysisResult:\n",
    "        result: ContentAnalysisResult()\n",
    "        copied_row = row.copy()\n",
    "        \n",
    "        if self.software == 'spacy':\n",
    "            result = self._anaylze_spacy(copied_row)\n",
    "        elif self.software == 'presidio':\n",
    "            result = self._analyze_presidio(copied_row)\n",
    "        else:\n",
    "            raise Exception(f'Software {self.software} is not supported')\n",
    "        \n",
    "        print(f'{self.software} based content analysis result: {result}')\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _anaylze_spacy(self, row) -> ContentAnalysisResult:\n",
    "        row['entities'] = self._get_entities(row['email_text'])\n",
    "        unique_entities = set()\n",
    "        for entities in row['entities']:\n",
    "            for ent in entities:\n",
    "                if len(ent) < 2:\n",
    "                    print(f'Entity {ent} is not valid')\n",
    "                    continue\n",
    "                unique_entities.add(ent[1])\n",
    "        row['sensitive'] =  [(ent[0], ent[1]) for ent in row['entities'] if ent[1] in ContentAnalyzer.SENSITIVE]\n",
    "        row['quids'] = [(ent[0], ent[1]) for ent in row['entities'] if ent[1] in ContentAnalyzer.QUIDS]\n",
    "        row['potentially_sensitive'] = [(ent[0], ent[1]) for ent in row['entities'] if ent[1] in ContentAnalyzer.POTENTIALLY_SENSITIVE]\n",
    "        spacy_result = self._predict_spacy_verdict(row)\n",
    "        \n",
    "        return spacy_result\n",
    "    \n",
    "    NON_SENSITIVE = ['EMAIL_ADDRESS', 'URL', 'ORG']\n",
    "    SENSITIVE = ['IP_ADDRESS', 'AU_ACN', 'US_ITIN', 'UK_NHS', 'AU_TFN', 'US_BANK_NUMBER', 'IN_PAN', 'US_DRIVER_LICENSE', 'IN_VEHICLE_REGISTRATION', 'SG_NRIC_FIN', 'US_SSN', 'US_PASSPORT', 'MEDICAL_LICENSE', 'PHONE_NUMBER']\n",
    "    QUASI_SENSITIVE = ['NRP', 'LOCATION', 'PERSON', 'DATE_TIME', 'GPE']\n",
    "    \n",
    "    @staticmethod\n",
    "    def _predict_presidio_verdict(row) -> ContentAnalysisResult:\n",
    "        result = ContentAnalysisResult()\n",
    "        \n",
    "        persons_counter = 0\n",
    "        if row is None:\n",
    "            return result\n",
    "        \n",
    "        email_text = row['email_text']\n",
    "        if email_text is None:\n",
    "            return result\n",
    "        sensitive_words = find_sensitive_words(row['email_text'])\n",
    "        phone_numbers = find_all_phone_numbers(email_text)\n",
    "        credit_cards = find_all_credit_cards(email_text)\n",
    "        ssns = find_all_ssn(email_text)\n",
    "        \n",
    "        for word in sensitive_words:\n",
    "            result.piis.append(word)\n",
    "        for phone_number in phone_numbers:\n",
    "            result.piis.append(phone_number)\n",
    "        for credit_card in credit_cards:\n",
    "            result.piis.append(credit_card)\n",
    "        for ssn in ssns:\n",
    "            result.piis.append(ssn)\n",
    "            \n",
    "    \n",
    "        if row['pii'] is None:\n",
    "            return result\n",
    "        \n",
    "        for pii in row['pii']:\n",
    "            if pii.score < 0.5:\n",
    "                continue\n",
    "            if pii.entity_type.startswith('IN_'):\n",
    "                continue\n",
    "            if pii.entity_type in ContentAnalyzer.NON_SENSITIVE:\n",
    "                continue\n",
    "            if pii.entity_type in ContentAnalyzer.SENSITIVE:\n",
    "                result.piis.append(pii)\n",
    "            if pii.entity_type in ContentAnalyzer.QUASI_SENSITIVE: \n",
    "                # todo maor - think about person\n",
    "                # if pii.entity_type == 'PERSON':\n",
    "                #     persons_counter += 1\n",
    "                #     if persons_counter > 2:\n",
    "                #         return 'Sensitive'\n",
    "                # else:\n",
    "                result.quids.append(pii)\n",
    "    \n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def build_pii_text(self, email_body_text, result):\n",
    "        start = result.start\n",
    "        end = result.end\n",
    "        pii_text = email_body_text[start:end]\n",
    "        return pii_text\n",
    "        \n",
    "    \n",
    "    counter = 0\n",
    "    def analyze_pii(self, email_body_text):\n",
    "        global counter\n",
    "        counter+= 1\n",
    "        if counter % 200 == 0:\n",
    "            print(f'Processed {counter} documents')\n",
    "        pii_list = []\n",
    "        email_body_text = email_body_text[:2000]\n",
    "        \n",
    "        try:\n",
    "            results = self.analyzer.analyze(text=email_body_text, language='en')\n",
    "        except Exception as e:\n",
    "            print(f'Error in document. Error: {e}')\n",
    "            return []\n",
    "        \n",
    "        for result in results:\n",
    "            pii_text = self.build_pii_text(email_body_text, result)\n",
    "            pii = PII(result.entity_type, result.score, pii_text)\n",
    "            pii_list.append(pii)\n",
    "    \n",
    "        return pii_list\n",
    "    \n",
    "\n",
    "\n",
    "    def _analyze_presidio(self, row) -> ContentAnalysisResult:\n",
    "        row['pii'] = row['email_text'].apply(lambda x: self.analyze_pii(x))\n",
    "        presidio_result = self._predict_presidio_verdict(row)\n",
    "        \n",
    "        return presidio_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.219810Z",
     "start_time": "2024-05-11T15:01:17.217240Z"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import pickle\n",
    "import os\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "class TopicAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model: LdaMulticore = LdaMulticore.load(os.path.join('lda', 'lda_model'))\n",
    "        self.dictionary: Dictionary = Dictionary.load(os.path.join('lda', 'dictionary'))\n",
    "\n",
    "        self.rule_to_topic_before =  {\n",
    "            'None' : [38, 35, 39, 17, 33],\n",
    "            '1.1' : [9, 35, 7, 20, 29,], # legal\n",
    "            '1.2': [1, 9, 7, 18, 20, 35, 24] + [5], # financial data\n",
    "            '1.3': [1, 35, 9, 7, 39] + [5], # business or financial\n",
    "            '2.1': [1, 9, 7, 35, 24, 20], # financial data\n",
    "            '2.2': [1, 9, 18, 7, 17, 16, 24] # business data\n",
    "        }\n",
    "\n",
    "        self.rule_to_topic =  { \n",
    "            'None' : [38, 33, 32, 19, 23, 40, 36, 8, 1, 4],  \n",
    "            '1.1' : [2, 20, 9, 35, 22, 25, 34, 5, 18, 37, 31, 28, 29, 14, 11], # legal\n",
    "            '1.2': [9, 34, 6, 11, 18, 37, 22, 16, 20, 35, 10, 7] , # financial data\n",
    "            '1.3': [35, 9, 20, 12, 23, 18, 39, 31, 4, 24, 10, 7, 28] + [14, 5, 13, 16], # business or financial\n",
    "            '2.1': [9, 2, 20, 34, 18, 25, 22, 24, 35, 37, 10, 7], # financial data\n",
    "            '2.2': [9, 18, 34, 20, 17, 24, 28, 16, 31, 39, 10, 7] # business data\n",
    "        }\n",
    "        \n",
    "    def predict(self, row) -> TopicAnalysisResult:\n",
    "        result = TopicAnalysisResult()\n",
    "        result.is_legal = self.is_legal(row)\n",
    "        result.is_business = self.is_business(row)\n",
    "        result.is_finance = self.is_finance(row)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def is_business(self, row):        \n",
    "        text = self.process_text(row)\n",
    "        return self._predict(text, '1.3') | self._predict(text, '2.3')\n",
    "        \n",
    "    \n",
    "    def is_finance(self, row):\n",
    "        text = self.process_text(row)\n",
    "        return self._predict(text, '1.2') | self._predict(text, '2.2') | self._predict(text, '1.3')\n",
    "    \n",
    "    def is_legal(self, row):\n",
    "        text = self.process_text(row)\n",
    "        return self._predict(text, '1.1')\n",
    "    \n",
    "    def process_text(self, row):\n",
    "        subject = row['Subject']\n",
    "        body = row['email_body']\n",
    "        text = self._text_to_bow(subject + \" \" + body)\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def _predict(self, text, label):\n",
    "        if label not in self.rule_to_topic.keys():\n",
    "            return False\n",
    "        \n",
    "        # clean_text = self._text_to_bow(text, self.dictionary)\n",
    "        topics_prob = self._get_top_n_probabilities(text, 4, 0.05)\n",
    "        if topics_prob is None or len(topics_prob) == 0:\n",
    "            return False\n",
    "        \n",
    "        topics = [t[0] for t in topics_prob]\n",
    "        count = len(self._intersecting_list(topics, self.rule_to_topic[label]))\n",
    "        count_none = len(self._intersecting_list(topics, self.rule_to_topic['None']))\n",
    "        if count > 0 and count_none < 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def _text_to_bow(self, text):\n",
    "    # Preprocess the text\n",
    "        preprocessed_text = preprocessing.preprocess_text(text, should_remove_links=True, should_remove_small_words=True, lemmatize=True, should_remove_digits=True) \n",
    "        # Convert the preprocessed text to a bag-of-words using the dictionary\n",
    "        bow = self.dictionary.doc2bow(preprocessed_text.split())\n",
    "        return bow\n",
    "\n",
    "    def _intersecting_list(self, list1, list2):\n",
    "        return list(set(list1) & set(list2))\n",
    "\n",
    "    def _get_probability_debug(self, row):\n",
    "        doc = self._text_to_bow(row['Subject'] + \" \" + row['email_body'])\n",
    "        topics =  self._get_top_n_probabilities(doc, 4, 0.08)\n",
    "        if topics is None:\n",
    "            return None\n",
    "        # if empty\n",
    "        if len(topics) == 0:\n",
    "            return None\n",
    "        return [t[0] for t in topics]\n",
    "\n",
    "    def _get_top_n_probabilities(self, doc, n=5, threshold=0.08):\n",
    "        if not doc:\n",
    "            return None  # Handle empty documents\n",
    "        topic_probs = self.model.get_document_topics(doc, minimum_probability=0)\n",
    "        if not topic_probs:\n",
    "            return None  # Handle cases with no significant topic probability\n",
    "        \n",
    "        topic_probs = [x for x in topic_probs if x[1] > threshold]\n",
    "        return sorted(topic_probs, key=lambda x: x[1], reverse=True)[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:17.453992Z",
     "start_time": "2024-05-11T15:01:17.222690Z"
    }
   },
   "outputs": [],
   "source": [
    "def _only_1_rules_are_violated(static_rule_violations_by_rule_id):\n",
    "    rule_2_prefix = '2.'\n",
    "    \n",
    "    for rule_id in static_rule_violations_by_rule_id.keys():\n",
    "        if rule_2_prefix in rule_id and static_rule_violations_by_rule_id[rule_id]:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def only_this_rule_violated(static_rule_violations_by_rule_id, rule_id):\n",
    "    return static_rule_violations_by_rule_id[rule_id] == True and sum(static_rule_violations_by_rule_id.values())\n",
    "\n",
    "\n",
    "def _pre_process(mail_row):\n",
    "    subject = mail_row['Subject']\n",
    "    body = mail_row['email_body']\n",
    "    text = preprocessing.preprocess_text(subject + \" \" + body, should_remove_small_words=True, should_remove_digits=False, lemmatize=True)\n",
    "    mail_row['email_text'] = text\n",
    "    return mail_row\n",
    "\n",
    "\n",
    "class Enforcer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.static_analyzer = StaticAnalyzer()\n",
    "        self.topic_analyzer = TopicAnalyzer()\n",
    "        self.content_analyzer = ContentAnalyzer()\n",
    "\n",
    "    def enforce(self, mail_row):\n",
    "        results = []\n",
    "        topic_analysis = self.topic_analyzer.predict(mail_row.copy())\n",
    "\n",
    "        if self.static_analyzer.is_violating_rule_1_1(mail_row):\n",
    "            if topic_analysis.is_legal:\n",
    "                results.append('1.1')\n",
    "        if self.static_analyzer.is_violating_rule_1_2(mail_row):\n",
    "            if topic_analysis.is_finance:\n",
    "                results.append('1.2')\n",
    "        if self.static_analyzer.is_violating_rule_1_3(mail_row):\n",
    "            if topic_analysis.is_business:\n",
    "                results.append('1.3')\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _enforce(self, mail_row) -> EnforcerResult:\n",
    "        result = EnforcerResult()\n",
    "        copied_row = mail_row.copy()\n",
    "        processed_row = _pre_process(copied_row) # not supoose to be in place, but each model should have its own preprocessing!!!\n",
    "        \n",
    "        static_rule_violations_by_rule_id: {str : bool} = self.analyze_static(processed_row)\n",
    "        content_analysis: ContentAnalysisResult\n",
    "        topic_analysis: TopicAnalysisResult\n",
    "    \n",
    "        # static analysis\n",
    "        if any(static_rule_violations_by_rule_id.values()) is False:\n",
    "            print(f'After static analysis, The Email is not violating any rule and therefore ALLOWED')\n",
    "            return result\n",
    "        \n",
    "        if only_this_rule_violated(static_rule_violations_by_rule_id, rule_id='2.3'):\n",
    "            print(f'Only rule 2.3 is potentially violated. therefore, we can skip topic analysis')\n",
    "            content_analysis: ContentAnalysisResult = self.content_analyzer.analyze(processed_row)\n",
    "            \n",
    "            if content_analysis.is_sensitive('2.3'):\n",
    "                print(f'Email is violating rule 2.3 due to content analysis: {content_analysis} and therefore BLOCKED')\n",
    "                result.violated_rules.append('2.3')\n",
    "            else:\n",
    "                print(f'Email is not violating any rule due to content analysis: {content_analysis} and therefore ALLOWED')\n",
    "            return result\n",
    "        \n",
    "        print(f'Will analyze topic for email, as all rules left required topic analysis')\n",
    "        topic_analysis = self.topic_analyzer.predict(mail_row.copy())\n",
    "        \n",
    "        if _only_1_rules_are_violated(static_rule_violations_by_rule_id):\n",
    "            content_analysis: ContentAnalysisResult = self.content_analyzer.analyze(processed_row) # todo maor - delete\n",
    "            print(f'Only Policy 1# rules are violated statically, can skip content analysis')\n",
    "        else:\n",
    "            print(f'Will analyze content for , as all rules left required content analysis (2)')\n",
    "            content_analysis: ContentAnalysisResult = self.content_analyzer.analyze(processed_row)\n",
    "        \n",
    "        for rule_id, static_rule_violation in static_rule_violations_by_rule_id.items():\n",
    "            if static_rule_violation:\n",
    "                print(f'Email is violating rule {rule_id} due to static analysis. Checking content analysis and topic analysis if needed')\n",
    "            \n",
    "                is_violated_rule_by_content_analysis = self._is_violating_rule_by_content(rule_id, content_analysis)\n",
    "                is_violated_rule_by_topic_analysis = self._is_violating_rule_by_topic(rule_id, topic_analysis)\n",
    "            \n",
    "                if is_violated_rule_by_content_analysis and is_violated_rule_by_topic_analysis:\n",
    "                    print(f'Email is violating rule {rule_id} due to content analysis or topic analysis and therefore BLOCKED')\n",
    "                    result.violated_rules.append(rule_id)\n",
    "                else:\n",
    "                    print(f'Email is not violating rule {rule_id} due to content analysis or topic analysis and therefore ALLOWED')\n",
    "            else:\n",
    "                print(f'Email is not violating rule {rule_id} due to static analysis')\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def analyze_static(self, processed_row) -> {str : bool}:\n",
    "        static_rule_violations_by_rule_id: {str: bool} = {\n",
    "            '1.1': self.static_analyzer.is_violating_rule_1_1(processed_row),\n",
    "            '1.2': self.static_analyzer.is_violating_rule_1_2(processed_row),\n",
    "            '1.3': self.static_analyzer.is_violating_rule_1_3(processed_row),\n",
    "            '2.1': self.static_analyzer.is_violating_rule_2_1(processed_row),\n",
    "            '2.2': self.static_analyzer.is_violating_rule_2_2(processed_row),\n",
    "            '2.3': self.static_analyzer.is_violating_rule_2_3(processed_row)\n",
    "        }\n",
    "        \n",
    "        return static_rule_violations_by_rule_id\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_violating_rule_by_content(rule_id, content_analysis: ContentAnalysisResult):\n",
    "        if '1.' in rule_id:\n",
    "            return False\n",
    "        if rule_id == '2.1' or rule_id == '2.2':\n",
    "            return content_analysis.is_sensitive\n",
    "        elif rule_id == '2.3':\n",
    "            return content_analysis.is_sensitive(rule_id)\n",
    "        else:\n",
    "            raise Exception(f'Rule {rule_id} is not supported')\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _is_violating_rule_by_topic(rule_id, topic_analysis: TopicAnalysisResult):\n",
    "        if rule_id == '1.1':\n",
    "            return topic_analysis.is_legal\n",
    "        elif rule_id == '1.2':\n",
    "            return topic_analysis.is_finance\n",
    "        elif rule_id == '1.3':\n",
    "            return topic_analysis.is_finance or topic_analysis.is_business\n",
    "        elif rule_id == '2.1':\n",
    "            return topic_analysis.is_finance\n",
    "        elif rule_id == '2.2':\n",
    "            return topic_analysis.is_business\n",
    "        elif rule_id == '2.3':\n",
    "            return False\n",
    "        else: \n",
    "            raise Exception(f'Rule {rule_id} is not supported for topic analysis')\n",
    "        \n",
    "enforcer = Enforcer()\n",
    "\n",
    "def classify_mail_extended(mail_row) -> EnforcerResult:\n",
    "    enforcer_results = enforcer.enforce(mail_row)\n",
    "    return enforcer_results\n",
    "\n",
    "def classify_mail(mail_row) -> bool:\n",
    "    enforcer_results = enforcer.enforce(mail_row)\n",
    "    is_allowed = enforcer_results.is_allowed()\n",
    "    verdict_prediction = 'ALLOWED' if is_allowed else 'BLOCKED'\n",
    "    print(f'Email Enforcer results: {enforcer_results}. Will be {verdict_prediction}')\n",
    "    return is_allowed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:19.850951Z",
     "start_time": "2024-05-11T15:01:17.454587Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_analyzer = TopicAnalyzer()\n",
    "\n",
    "\n",
    "test_df = enron_df[enron_df['violated_rules'].apply(lambda x: '1.3' in x)].sample(frac=0.01, random_state=42)\n",
    "test_df['is_allowed'] = test_df.apply(lambda row: classify_mail_extended(row), axis=1)\n",
    "test_df['topics'] = test_df.apply(lambda row: topic_analyzer._get_probability_debug(row), axis=1)\n",
    "\n",
    "test_df.head(10)\n",
    "\n",
    "# todo maor - Print accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_outside'] = test_df.apply(lambda x: is_leaving_corporate(x), axis=1)\n",
    "\n",
    "print(len(test_df[(test_df['is_outside'] == True) & test_df['is_allowed'].apply(lambda x: '1.3' not in x) ]))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize statistical data\n",
    "# 1.1\n",
    "labeled_data = enron_df[enron_df['violated_rules'].apply(lambda x: '1.1' in x)]\n",
    "total = len(labeled_data)\n",
    "print(f'Total 1.1: {total}')\n",
    "print(f'1.1 catched: {len(labeled_data[labeled_data[\"is_legal\"] == True])}')\n",
    "\n",
    "# 1.2\n",
    "labeled_data = enron_df[enron_df['violated_rules'].apply(lambda x: '1.2' in x)]\n",
    "print(f'Total 1.2: {len(labeled_data)}')\n",
    "print(f'1.2 catched {len(labeled_data[labeled_data['is_EU_To_NA']])}')\n",
    "\n",
    "# 1.3\n",
    "labeled_data = enron_df[enron_df['violated_rules'].apply(lambda x: '1.3' in x)]\n",
    "print(f'Total 1.2: {len(labeled_data)}')\n",
    "labeled_data['is_leaving_corp'] = labeled_data.apply(lambda x: is_leaving_corporate(x))\n",
    "print(f'1.2 catched {len(labeled_data[labeled_data['is_EU_To_NA']])}')\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dictionary = topic_analyzer.model, topic_analyzer.dictionary\n",
    "enron_df['email_text'] = enron_df['Subject'] + \" \" + enron_df['email_body']\n",
    "preprocessing.data_cleaning(enron_df, True)\n",
    "enron_df['tokens'] = enron_df['email_text'].apply(lambda x: x.split())\n",
    "enron_df['bow'] = enron_df['tokens'].apply(dictionary.doc2bow)\n",
    "\n",
    "bow_corpus = enron_df[enron_df['bow'].map(len) > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:01:19.852680Z",
     "start_time": "2024-05-11T15:01:19.851561Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "vis = gensimvis.prepare(model, bow_corpus['bow'], dictionary)\n",
    "pyLDAvis.display(vis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
