{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "enron_df = pd.read_pickle('enron_students.pkl')\n",
    "\n",
    "# # Read dictionary pkl file\n",
    "# with open('email_to_departments.pkl', 'rb') as fp:\n",
    "#     email_to_departments = pickle.load(fp)\n",
    "#     print('email_to_departments dictionary loaded from pkl file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "enron_df['To'] = enron_df['To'].fillna('')\n",
    "enron_df['From'] = enron_df['From'].fillna('')\n",
    "enron_df['X-From'] = enron_df['X-From'].fillna('')\n",
    "enron_df['X-To'] = enron_df['X-To'].fillna('')\n",
    "enron_df['X-cc'] = enron_df['X-cc'].fillna('')\n",
    "enron_df['X-bcc'] = enron_df['X-bcc'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_emails(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # Regex to match email addresses\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "def extract_single_email_from_text(text):\n",
    "    emails = extract_emails(text)\n",
    "    if (len(emails) > 1):\n",
    "        print(f\"Multiple emails found in: {text}\")\n",
    "    elif len(emails) == 1:\n",
    "        return emails[0]\n",
    "    print(f\"Could not extract email from: {text}\")\n",
    "    return None\n",
    "\n",
    "def extract_sent_to_emails(text):\n",
    "    emails = extract_emails(text)\n",
    "    if len(emails) > 0:\n",
    "        return emails\n",
    "    return []\n",
    "\n",
    "def is_between_ect_and_ees(email_to_depart_dict, row, is_including_cc_bcc=False, additional_check=True):\n",
    "    sender_email = extract_single_email_from_text(row['From'])\n",
    "    if sender_email in email_to_depart_dict:\n",
    "        sender_department = email_to_depart_dict[sender_email]\n",
    "        if sender_department != 'ECT' and sender_department != 'EES':\n",
    "            return False\n",
    "        else:\n",
    "            if is_including_cc_bcc:\n",
    "                reciever_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To'])\n",
    "            else:\n",
    "                reciever_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To']) + extract_sent_to_emails(row['X-cc']) + extract_sent_to_emails(row['X-bcc'])\n",
    "            \n",
    "            for email in reciever_emails:\n",
    "                if email in email_to_depart_dict:\n",
    "                    reciever_department = email_to_depart_dict[email]\n",
    "\n",
    "                    if sender_department == 'ECT' and reciever_department == 'EES':\n",
    "                        return True\n",
    "                    elif sender_department == 'EES' and reciever_department == 'ECT':\n",
    "                        return True\n",
    "                    \n",
    "            if additional_check:\n",
    "                if 'ECT' in row['X-To']:\n",
    "                    if 'EES' in row['X-From']:\n",
    "                        return True\n",
    "                if 'EES' in row['X-To']:\n",
    "                    if 'ECT' in row['X-From']:\n",
    "                        return True\n",
    "      \n",
    "            return False\n",
    "    \n",
    "    else:\n",
    "        if additional_check:\n",
    "            if 'ECT' in row['X-To']:\n",
    "                if 'EES' in row['X-From']:\n",
    "                    return True\n",
    "            if 'EES' in row['X-To']:\n",
    "                if 'ECT' in row['X-From']:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dict():\n",
    "    email_to_departments = {}\n",
    "    email_to_departments.update({\n",
    "            'smith.day@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stuart.zisman@enron.com' : 'ECT'\n",
    "            ,'melissa.murphy@enron.com' : 'ECT'\n",
    "            ,'thresa.allen@enron.com' : 'ECT'\n",
    "            ,'leslie.reeves@enron.com' : 'ECT'\n",
    "            ,'stacey.white@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'brad.nebergall@enron.com': 'ECT'\n",
    "            ,'colleen.koenig@enron.com': 'EES'\n",
    "            ,'john.kinser@enron.com': 'ECT'\n",
    "            ,'jeff.merola@enron.com': 'EES'\n",
    "            })\n",
    "    \n",
    "    email_to_departments.update({'eric.bass@enron.com': 'ECT'})\n",
    "\n",
    "    multiple_count = 0\n",
    "    # TODO: implement check for multiple emails in X-From and From. should be only one email\n",
    "    for index, row in enron_df.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        from_emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        to_emails = set([mail.lower() for mail in (extract_emails(row['To']) + extract_emails(row['X-To']))])\n",
    "        if len(from_emails) == 0:\n",
    "            continue\n",
    "        if len(from_emails) > 1:\n",
    "            from_emails = [mail for mail in from_emails if not mail.startswith('imceanotes')]\n",
    "            if len(from_emails) > 1 or len(from_emails) == 0:\n",
    "                multiple_count += 1\n",
    "\n",
    "        for email in from_emails:\n",
    "            email = email.lower()\n",
    "            # TODO: validate company email starts with enron.com\n",
    "            if \"enron.com\" not in email and email not in email_to_departments:\n",
    "                email_to_departments.update({email: 'NA'})\n",
    "            else:\n",
    "                if email in email_to_departments and email_to_departments[email] != 'Other':\n",
    "                    continue\n",
    "                elif 'EES' in row['X-From']:\n",
    "                    email_to_departments.update({email: 'EES'})\n",
    "                    if len(to_emails) == 1:\n",
    "                        email_to_departments.update({list(to_emails)[0]: 'ECT'})\n",
    "                elif 'ECT' in row['X-From']:\n",
    "                    email_to_departments.update({email: 'ECT'})\n",
    "                    if len(to_emails) == 1:\n",
    "                        email_to_departments.update({list(to_emails)[0]: 'EES'})\n",
    "                else:\n",
    "                    email_to_departments.update({email: 'Other'})\n",
    "\n",
    "        # new addition - need to check\n",
    "        if len(to_emails) == 1:\n",
    "            if 'ECT' in row['X-To']:\n",
    "                email_to_departments.update({list(to_emails)[0]: 'ECT'})\n",
    "            elif 'EES' in row['X-To']:\n",
    "                email_to_departments.update({list(to_emails)[0]: 'EES'})\n",
    "\n",
    "    email_to_departments.update({\n",
    "            'smith.day@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stuart.zisman@enron.com' : 'ECT'\n",
    "            ,'melissa.murphy@enron.com' : 'ECT'\n",
    "            ,'thresa.allen@enron.com' : 'ECT'\n",
    "            ,'leslie.reeves@enron.com' : 'ECT'\n",
    "            ,'stacey.white@enron.com' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'brad.nebergall@enron.com': 'ECT'\n",
    "            ,'colleen.koenig@enron.com': 'EES'\n",
    "            ,'john.kinser@enron.com': 'ECT'\n",
    "            ,'jeff.merola@enron.com\t': 'EES'\n",
    "            ,'scott.mills@enron.com': 'ECT'\n",
    "            ,'marilyn.colbert@enron.com': 'ECT'\n",
    "            ,'molly.harris@enron.com': 'ECT'\n",
    "            ,'joseph.wagner@enron.com': 'ECT'\n",
    "            ,'stuart.zisman@enron.com,' : 'ECT'\n",
    "            ,'janet.wallis@enron.com' : 'ECT'\n",
    "            ,'daren.farmer@enron.com' : 'ECT'\n",
    "            ,'stephanie.gardner@enron.com' : 'EES'\n",
    "\n",
    "            })\n",
    "    \n",
    "    return email_to_departments\n",
    "\n",
    "\n",
    "email_to_department_dict = build_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20638"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(email_to_department_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract email from: u@d.h\n",
      "Could not extract email from: pep <performance.>\n"
     ]
    }
   ],
   "source": [
    "enron_df['is_between_ect_and_ees'] = enron_df.apply(lambda row: is_between_ect_and_ees(email_to_department_dict, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all spaces \n",
    "enron_df['violated_rules'] = enron_df['violated_rules'].apply(lambda x: x.replace(' ', ''))\n",
    "\n",
    "# apply split ',' on violated rules\n",
    "enron_df['violated_rules'] = enron_df['violated_rules'].apply(lambda x: x.split(',')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(enron_df_not_catched_1_1, email_to_depart):\n",
    "    # iterate over the rows of the dataframe enron_df_not_catched_1_1\n",
    "    updated = 0\n",
    "    for index, row in enron_df_not_catched_1_1.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        sent_to_emails = extract_sent_to_emails(row['To']) + extract_sent_to_emails(row['X-To'])\n",
    "        if len(emails) == 0:\n",
    "            continue\n",
    "        if len(emails) > 1:\n",
    "            emails = [mail for mail in emails if not mail.startswith('imceanotes')]\n",
    "            if len(emails) > 1 or len(emails) == 0:\n",
    "                # print(f\"From: {row['From']} X-From: {row['X-From']} has multiple emails: {emails}\")\n",
    "                continue\n",
    "        for email in emails:\n",
    "            email = email.lower()\n",
    "            # if from email is in dictionary, get the department\n",
    "            if email in email_to_depart:\n",
    "                depart = email_to_depart[email]\n",
    "                if depart == 'ECT':\n",
    "                    if len(sent_to_emails) == 1:\n",
    "                        email_to_depart[sent_to_emails[0]] = 'EES'\n",
    "                        updated += 1\n",
    "                elif depart == 'EES':\n",
    "                    if len(sent_to_emails) == 1:\n",
    "                        email_to_depart[sent_to_emails[0]] = 'ECT'\n",
    "                        updated += 1\n",
    "\n",
    "    print(f\"Updated {updated} emails\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 198084 emails\n"
     ]
    }
   ],
   "source": [
    "enron_df_not_catched_1_1 = enron_df[enron_df['is_between_ect_and_ees'] == False & enron_df['violated_rules'].apply(lambda x: '1.1' in x)]\n",
    "\n",
    "update_dict(enron_df_not_catched_1_1, email_to_department_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HERE WE START USER TO LOCATION *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_to_location_dict(df):\n",
    "    email_to_location = {}\n",
    "    for index, row in enron_df.iterrows():\n",
    "        x_emails = extract_emails(row['X-From'])\n",
    "        from_emails = set([mail.lower() for mail in (extract_emails(row['From']) + x_emails)])\n",
    "        if '/OU=EU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'EU'})\n",
    "        elif '/HOU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        elif '/OU=NA/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "\n",
    "    # iterate over the dataframe to get \n",
    "    violated_rule_1_2 = df[df['violated_rules'].apply(lambda x: '1.2' in x)]\n",
    "\n",
    "    for index, row in violated_rule_1_2.iterrows():\n",
    "        from_emails = extract_emails(row['X-From'])\n",
    "        from_emails = list(set([mail.lower() for mail in (extract_emails(row['From']) + from_emails)]))\n",
    "        if '/OU=EU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'EU'})\n",
    "        elif '/HOU/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        elif '/OU=NA/' in row['X-From']:\n",
    "            for email in from_emails:\n",
    "                email_to_location.update({email: 'NA'})\n",
    "        \n",
    "        sent_to_email = extract_emails(row['X-To'])\n",
    "        if len(sent_to_email) == 1:\n",
    "            if from_emails[0] not in email_to_location:\n",
    "                continue\n",
    "            if email_to_location[from_emails[0]] == 'EU':\n",
    "                email_to_location.update({sent_to_email[0]: 'NA'})\n",
    "            elif email_to_location[from_emails[0]] == 'NA':\n",
    "                email_to_location.update({sent_to_email[0]: 'EU'})\n",
    "            \n",
    "    return email_to_location\n",
    "\n",
    "\n",
    "email_to_location = get_email_to_location_dict(enron_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_between_EU_and_NA(row, email_to_location):\n",
    "    from_email = extract_emails(row['From'])\n",
    "    from_email = extract_emails(row['X-From']) + from_email\n",
    "    from_email = list(set([mail.lower() for mail in from_email]))\n",
    "    to_email = extract_emails(row['To'])\n",
    "    to_email = extract_emails(row['X-To']) + to_email\n",
    "    to_email = list(set([mail.lower() for mail in to_email]))\n",
    "    if len(from_email) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "    if from_email[0] in email_to_location:\n",
    "        if email_to_location[from_email[0]] == 'EU':\n",
    "            for email in to_email:\n",
    "                if email in email_to_location and email_to_location[email] == 'NA':\n",
    "                    return True\n",
    "        if email_to_location[from_email[0]] == 'NA':\n",
    "            for email in to_email:\n",
    "                if email in email_to_location and email_to_location[email] == 'EU':\n",
    "                    return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df['is_EU_To_NA'] = enron_df.apply(lambda row: is_between_EU_and_NA(row, email_to_location), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enron_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m not_working \u001b[38;5;241m=\u001b[39m \u001b[43menron_df\u001b[49m[(enron_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_EU_To_NA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (enron_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviolated_rules\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.2\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x))]\n\u001b[0;32m      2\u001b[0m not_working\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enron_df' is not defined"
     ]
    }
   ],
   "source": [
    "not_working = enron_df[(enron_df['is_EU_To_NA'] == False) & (enron_df['violated_rules'].apply(lambda x: '1.2' in x))]\n",
    "not_working.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985\n"
     ]
    }
   ],
   "source": [
    "print(len(email_to_location))\n",
    "# enron_df.head(10)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_suit_dict():\n",
    "    emails = set({\n",
    "        'kenneth.lay@enron.com',\n",
    "        'ken.skilling@enron.com',\n",
    "        'rbowers@nyiso.com',\n",
    "        'michael.brown@enron.com',\n",
    "        'john.sherriff@enron.com',\n",
    "        'jeffrey.sherrick@enron.com',\n",
    "        'a..shankman@enron.com',\n",
    "        'ken.rice@enron.com',\n",
    "        'greg.piper@enron.com',\n",
    "        'mark.metts@enron.com',\n",
    "        'coo.jeff@enron.com',\n",
    "        'rebecca.mcdonald@enron.com',\n",
    "        'danny.mccarty@enron.com',\n",
    "        'dan.leff@enron.com',\n",
    "        'john.lavorato@enron.com',\n",
    "        'mark.koenig@enron.com',\n",
    "        'louise.kitchen@enron.com',\n",
    "        'stanley.horton@enron.com',\n",
    "        '40enron@enron.com', # for some reason is tagged for stanley horton\n",
    "        'ben.glisan@enron.com',\n",
    "        'mark.frevert@enron.com',\n",
    "        'andrew.fastow@enron.com',\n",
    "        'jr..legal@enron.com',\n",
    "        'derrick@enron.com',\n",
    "        'david.delainey@enron.com',\n",
    "        'richard.causey@enron.com',\n",
    "        'michael.brown@enron.com',\n",
    "        'raymond.bowen@enron.com',\n",
    "    })\n",
    "\n",
    "    violated_rule_1_3 = enron_df[enron_df['violated_rules'].apply(lambda x: '2.2' in x)]\n",
    "\n",
    "    for index, row in violated_rule_1_3.iterrows():\n",
    "        from_emails = extract_emails(row['X-From'])\n",
    "        from_emails = list(set([mail.lower() for mail in (extract_emails(row['From']) + from_emails)]))\n",
    "        to_email = extract_emails(row['To'])\n",
    "        to_email = extract_emails(row['X-To']) + to_email\n",
    "        to_email = list(set([mail.lower() for mail in to_email]))\n",
    "\n",
    "        if len(from_emails) == 0:\n",
    "            continue\n",
    "        for email in from_emails:\n",
    "            emails.add(email)\n",
    "            # print(f\"Email: {email} is in C-Suit\")\n",
    "\n",
    "        # for email in to_email:\n",
    "        #     emails.add(email)\n",
    "        #     print(f\"Email: {email} is in C-Suit\")\n",
    "\n",
    "    return emails\n",
    "\n",
    "c_suit_emails = get_c_suit_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_c_suit(row, c_suit_emails):\n",
    "    # Extract and clean email addresses from 'From' fields\n",
    "    from_emails = extract_emails(row['X-From']) + extract_emails(row['From'])\n",
    "    from_emails = list(set(email.lower() for email in from_emails))\n",
    "\n",
    "    # Extract and clean email addresses from 'To' fields\n",
    "    to_email = extract_emails(row['X-To']) + extract_emails(row['To'])\n",
    "    to_email = list(set(email.lower() for email in to_email))\n",
    "\n",
    "    # Check if both sender and recipient are in the c-suite email list\n",
    "    return any(email in c_suit_emails for email in from_emails) and any(email in c_suit_emails for email in to_email)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df['is_c_suit'] = enron_df.apply(lambda row: is_in_c_suit(row, c_suit_emails), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_to_emails(row):\n",
    "    from_emails = extract_emails(row['X-From']) + extract_emails(row['From'])\n",
    "    from_emails = list(set(email.lower() for email in from_emails))\n",
    "\n",
    "    # Extract and clean email addresses from 'To' fields\n",
    "    to_email = extract_emails(row['X-To']) + extract_emails(row['To'])\n",
    "    to_email = list(set(email.lower() for email in to_email))\n",
    "\n",
    "    return from_emails, to_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaving_corporate(row):\n",
    "    from_emails, to_email = get_from_to_emails(row)\n",
    "    if len(from_emails) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "    \n",
    "    for email in to_email:\n",
    "        if 'enron.com' not in email:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_leaving_ect(row, email_to_depart):\n",
    "    from_emails, to_email = get_from_to_emails(row)\n",
    "    if len(from_emails) == 0 or len(to_email) == 0:\n",
    "        return False\n",
    "\n",
    "    for email in from_emails:\n",
    "        if email in email_to_depart and email_to_depart[email] == 'ECT':\n",
    "            for t_email in to_email:\n",
    "                if t_email in email_to_depart and email_to_depart[t_email] != 'ECT':\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticAnalyzer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_1(row):\n",
    "        return is_between_ect_and_ees(email_to_department_dict, row, is_including_cc_bcc=False, additional_check=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_2(row):\n",
    "        return is_between_EU_and_NA(row, email_to_location)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_1_3(row):\n",
    "        return is_leaving_corporate(row)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_1(row):\n",
    "        return is_leaving_ect(row, email_to_department_dict)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_2(row):\n",
    "        return is_in_c_suit(row, c_suit_emails)\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_violating_rule_2_3(row):\n",
    "        return is_leaving_corporate(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class ContentAnalysisResult:\n",
    "    quids = []\n",
    "    piis = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'(Sensitive: {len(self.piis) > 0 or len(self.quids) > 0} | Quids: {self.quids} | PII: {self.piis})'\n",
    "    \n",
    "    def is_sensitive(self, rule_id):\n",
    "        if rule_id == '2.3':\n",
    "            return len(self.piis) > 0 or len(self.quids) >= 2\n",
    "        else:\n",
    "            return len(self.piis) > 0 or len(self.quids) > 0\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class TopicAnalysisResult:\n",
    "    is_legal: bool = False\n",
    "    is_business: bool = False\n",
    "    is_finance: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'(Legal: {self.is_legal} | Business: {self.is_business} | Finance: {self.is_finance})'\n",
    "    \n",
    "@dataclasses.dataclass()\n",
    "class EnforcerResult:\n",
    "    violated_rules = []\n",
    "    \n",
    "    def is_allowed(self)-> bool:\n",
    "        return len(self.violated_rules) == 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Violated Rules: {self.violated_rules}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "\n",
    "\n",
    "def find_all_ssn(text):\n",
    "    return re.findall(r'\\d{3}-\\d{2}-\\d{4}', text)\n",
    "\n",
    "def find_all_credit_cards(text):\n",
    "    return re.findall(r'\\d{4}-\\d{4}-\\d{4}-\\d{4}', text) \n",
    "\n",
    "def find_all_phone_numbers(text):\n",
    "    return re.findall(r'\\(?\\d{3}\\)?\\s*-\\s*\\d{3}\\s*-\\s*\\d{4}', text) \n",
    "\n",
    "def find_sensitive_words(text):\n",
    "    return re.findall(r'password|attach|confidential', text.lower())\n",
    "\n",
    "class ContentAnalyzer:\n",
    "    \n",
    "    class PII:\n",
    "        def __init__(self, entity_type, score, text):\n",
    "            self.entity_type = entity_type\n",
    "            self.score = score\n",
    "            self.text = text\n",
    "            \n",
    "        def __str__(self):\n",
    "            return f'(Entity: {self.entity_type} | Score: {self.score} | Text: {self.text})'\n",
    "    \n",
    "    MAX_DOC_SIZE_FOR_SPACY = 2000\n",
    "    QUIDS = ['ORG', 'GPE', 'LOW', 'FAC', 'LOC']\n",
    "    SENSITIVE = ['MONEY', 'PERCENT', 'NORP', 'PRODUCT', 'EVENT']\n",
    "    POTENTIALLY_SENSITIVE = ['DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL', 'PERSON']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.nlp.max_length = 1500000\n",
    "    \n",
    "    def _get_entities(self, document):\n",
    "        try:\n",
    "            doc_len = len(document)\n",
    "            if doc_len > self.MAX_DOC_SIZE_FOR_SPACY:\n",
    "                document = document[:self.MAX_DOC_SIZE_FOR_SPACY]\n",
    "            doc = self.nlp(document)\n",
    "            return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "        except ValueError as e :\n",
    "            print(f'Error in document. Error: {e}')\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def predict_spacy_verdict(self, row) -> ContentAnalysisResult:\n",
    "        result = ContentAnalysisResult()\n",
    "        count_persons = 0\n",
    "        if len(find_sensitive_words(row['email_text'])) > 0:\n",
    "            result.i\n",
    "        if row['quids'] and len(row['quids']) > 0: \n",
    "            return 'Sensitive'\n",
    "        elif row['sensitive'] and len(row['sensitive']) > 0:\n",
    "            return 'Sensitive'\n",
    "        elif row['potentially_sensitive'] and len(row['potentially_sensitive']) > 0:\n",
    "            for ent in row['potentially_sensitive']:\n",
    "                if ent[1] == 'DATE':\n",
    "                    continue\n",
    "                elif ent[1] == 'TIME':\n",
    "                    continue\n",
    "                elif ent[1] == 'QUANTITY':\n",
    "                    continue\n",
    "                elif ent[1] == 'ORDINAL':\n",
    "                    continue\n",
    "                elif ent[1] == 'CARDINAL':\n",
    "                    if len(find_all_phone_numbers(ent[0])) > 0 or len(find_all_credit_cards(ent[0])) > 0 or len(find_all_ssn(ent[0])) > 0:\n",
    "                        return 'Sensitive'\n",
    "                elif ent[1] == 'PERSON':\n",
    "                    count_persons += 1\n",
    "                    if count_persons > 2:\n",
    "                        return 'Sensitive'\n",
    "                    continue\n",
    "            return 'Non-sensitive'\n",
    "        else:\n",
    "            return 'Non-sensitive'\n",
    "    \n",
    "    def analyze(self, row: pd.DataFrame) -> ContentAnalysisResult:\n",
    "        result = ContentAnalysisResult()\n",
    "        copied_row = row.copy()\n",
    "        copied_row['entities'] = copied_row['email_text'].apply(self._get_entities)\n",
    "        unique_entities = set()\n",
    "    \n",
    "        for entities in copied_row['entities']:\n",
    "            for ent in entities:\n",
    "                unique_entities.add(ent[1])\n",
    "                \n",
    "        copied_row['sensitive'] = copied_row['entities'].apply(lambda x: [(ent[0], ent[1]) for ent in x if ent[1] in ContentAnalyzer.SENSITIVE])\n",
    "        copied_row['quids'] = copied_row['entities'].apply(lambda x: [(ent[0], ent[1]) for ent in x if ent[1] in ContentAnalyzer.QUIDS])\n",
    "        copied_row['potentially_sensitive'] = copied_row['entities'].apply(lambda x: [(ent[0], ent[1]) for ent in x if ent[1] in ContentAnalyzer.POTENTIALLY_SENSITIVE])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import os\n",
    "\n",
    "class TopicAnalyzer:\n",
    "    # todo efi\n",
    "    def __init__(self):\n",
    "        self.model: LdaMulticore = LdaMulticore.load(os.path.join('lda', 'lda_model'))\n",
    "        self.dictionary: Dictionary = Dictionary.load(os.path.join('lda', 'dictionary'))\n",
    "        self.rule_to_topic =  {\n",
    "            'None' : [38, 35, 39, 17, 33],\n",
    "            '1.1' : [9, 35, 7, 20, 29], # legal\n",
    "            '1.2': [1, 9, 7, 18, 20, 35, 24] + [5], # financial data\n",
    "            '1.3': [1, 35, 9, 7, 39] + [5], # business or financial\n",
    "            '2.1': [1, 9, 7, 35, 24, 20], # financial data\n",
    "            '2.2': [1, 9, 18, 7, 17, 16, 24] # business data\n",
    "        }\n",
    "        \n",
    "    def predict(self, row) -> TopicAnalysisResult:\n",
    "        result = TopicAnalysisResult()\n",
    "        result.is_legal = self.is_legal(row)\n",
    "        result.is_business = self.is_business(row)\n",
    "        result.is_finance = self.is_finance(row)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def is_business(self, row):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def is_finance(self, row):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def is_legal(self, row):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T10:57:02.913213Z",
     "start_time": "2024-05-11T10:57:02.902938Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 41 (4084019371.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 44\u001b[0;36m\u001b[0m\n\u001b[0;31m    def only_this_rule_violated(self, static_rule_violations_by_rule_id, rule_id):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 41\n"
     ]
    }
   ],
   "source": [
    "def _only_1_rules_are_violated(static_rule_violations_by_rule_id):\n",
    "    rule_2_prefix = '2.'\n",
    "    \n",
    "    for rule_id, rule_violation in static_rule_violations_by_rule_id:\n",
    "        if rule_id.contains(rule_2_prefix) and static_rule_violations_by_rule_id[rule_id]:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def only_this_rule_violated(static_rule_violations_by_rule_id, rule_id):\n",
    "    return static_rule_violations_by_rule_id[rule_id] == True and sum(static_rule_violations_by_rule_id.values())\n",
    "\n",
    "\n",
    "class Enforcer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.static_analyzer = StaticAnalyzer()\n",
    "        self.topic_analyzer = TopicAnalyzer()\n",
    "        self.content_analyzer = ContentAnalyzer()\n",
    "        \n",
    "    def _pre_process(self, mail_row):\n",
    "        # todo efi\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def enforce(self, mail_row) -> EnforcerResult:\n",
    "        result = EnforcerResult()\n",
    "        processed_row = self._pre_process(mail_row)\n",
    "        \n",
    "        static_rule_violations_by_rule_id: {str : bool} = self.analyze_static(processed_row)\n",
    "        content_analysis: ContentAnalysisResult = None\n",
    "        topic_analysis: TopicAnalysisResult = None\n",
    "    \n",
    "        # static analysis\n",
    "        if any(static_rule_violations_by_rule_id.values()) is False:\n",
    "            print(f'After static analysis, The Email {mail_row} is not violating any rule and therefore ALLOWED')\n",
    "            return result\n",
    "        \n",
    "        if only_this_rule_violated(static_rule_violations_by_rule_id, rule_id='2,3'):\n",
    "            print(f'Only rule 2.3 is potentially violated in {mail_row}, therefore, we can skip topic analysis')\n",
    "            content_analysis: ContentAnalysisResult = self.content_analyzer.analyze(processed_row)\n",
    "            \n",
    "            if content_analysis.is_sensitive('2.3'):\n",
    "                print(f'Email {mail_row} is violating rule 2.3 due to content analysis: {content_analysis} and therefore BLOCKED')\n",
    "                result.violated_rules.append('2.3')\n",
    "            else:\n",
    "                print(f'Email {mail_row} is not violating any rule due to content analysis: {content_analysis} and therefore ALLOWED')\n",
    "            return result\n",
    "        \n",
    "        print(f'Will analyze topic for {mail_row}, as all rules left required topic analysis')\n",
    "        topic_analysis = self.topic_analyzer.predict(processed_row)\n",
    "        \n",
    "        if _only_1_rules_are_violated(static_rule_violations_by_rule_id):\n",
    "            print(f'Only Policy 1# rules are violated statically, can skip content analysis')\n",
    "        else:\n",
    "            print(f'Will analyze content for {mail_row}, as all rules left required content analysis (2)')\n",
    "            content_analysis: ContentAnalysisResult = self.content_analyzer.analyze(processed_row)\n",
    "        \n",
    "        for rule_id, static_rule_violation in static_rule_violations_by_rule_id.items():\n",
    "            if static_rule_violation:\n",
    "                print(f'Email {mail_row} is violating rule {rule_id} due to static analysis. Checking content analysis and topic analysis if needed')\n",
    "            \n",
    "                is_violated_rule_by_content_analysis = self._is_violating_rule_by_content(rule_id, content_analysis)\n",
    "                is_violated_rule_by_topic_analysis = self._is_violating_rule_by_topic(rule_id, topic_analysis)\n",
    "            \n",
    "                if is_violated_rule_by_content_analysis and is_violated_rule_by_topic_analysis:\n",
    "                    print(f'Email {mail_row} is violating rule {rule_id} due to content analysis or topic analysis and therefore BLOCKED')\n",
    "                    result.violated_rules.append(rule_id)\n",
    "                else:\n",
    "                    print(f'Email {mail_row} is not violating rule {rule_id} due to content analysis or topic analysis and therefore ALLOWED')\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def analyze_static(self, processed_row) -> {str : bool}:\n",
    "        static_rule_violations_by_rule_id: {str: bool} = {\n",
    "            '1.1': self.static_analyzer.is_violating_rule_1_1(processed_row),\n",
    "            '1.2': self.static_analyzer.is_violating_rule_1_2(processed_row),\n",
    "            '1.3': self.static_analyzer.is_violating_rule_1_3(processed_row),\n",
    "            '2.1': self.static_analyzer.is_violating_rule_2_1(processed_row),\n",
    "            '2.2': self.static_analyzer.is_violating_rule_2_2(processed_row),\n",
    "            '2.3': self.static_analyzer.is_violating_rule_2_3(processed_row)\n",
    "        }\n",
    "        \n",
    "        return static_rule_violations_by_rule_id\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_violating_rule_by_content(rule_id, content_analysis: ContentAnalysisResult):\n",
    "        if rule_id.contains('1.'):\n",
    "            return False\n",
    "        if rule_id is '2.1' or rule_id is '2.2':\n",
    "            return content_analysis.is_sensitive\n",
    "        elif rule_id is '2.3':\n",
    "            return content_analysis.is_sensitive(rule_id)\n",
    "        else:\n",
    "            raise Exception(f'Rule {rule_id} is not supported')\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _is_violating_rule_by_topic(rule_id, topic_analysis: TopicAnalysisResult):\n",
    "        if rule_id is '1.1':\n",
    "            return topic_analysis.is_legal\n",
    "        elif rule_id is '1.2':\n",
    "            return topic_analysis.is_finance\n",
    "        elif rule_id is '1.3':\n",
    "            return topic_analysis.is_finance or topic_analysis.is_business\n",
    "        elif rule_id is '2.1':\n",
    "            return topic_analysis.is_finance\n",
    "        elif rule_id is '2.2':\n",
    "            return topic_analysis.is_business\n",
    "        elif rule_id is '2.3':\n",
    "            return False\n",
    "        else: \n",
    "            raise Exception(f'Rule {rule_id} is not supported for topic analysis')\n",
    "        \n",
    "\n",
    "\n",
    "enforcer = Enforcer()\n",
    "\n",
    "def classify_mail(mail_row) -> bool:\n",
    "    enforcer_results = enforcer.enforce(mail_row)\n",
    "    \n",
    "    pass\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
