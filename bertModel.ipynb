{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y9O2GCP3O1H",
    "outputId": "ca9bcf6d-80ea-4739-cc68-a3a7e3d12b3f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "  from google.colab import drive \n",
    "  drive.mount('/content/drive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNl1k2q43O1L",
    "outputId": "0d412a99-2185-4f32-e4b5-a6c12726ae5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517401 entries, 0 to 517400\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Date            517401 non-null  object\n",
      " 1   From            517401 non-null  object\n",
      " 2   To              495554 non-null  object\n",
      " 3   X-To            517372 non-null  object\n",
      " 4   X-From          517372 non-null  object\n",
      " 5   X-cc            517372 non-null  object\n",
      " 6   X-bcc           517372 non-null  object\n",
      " 7   Subject         517401 non-null  object\n",
      " 8   email_body      517401 non-null  object\n",
      " 9   verdict         517401 non-null  object\n",
      " 10  violated_rules  517401 non-null  object\n",
      "dtypes: object(11)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "  enron_df = pd.read_pickle('/content/drive/MyDrive/Study/Cyber_AI/enron_students.pkl')\n",
    "else:\n",
    "  enron_df = pd.read_pickle('enron_students.pkl')\n",
    "enron_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "xJXVgySr3O1M"
   },
   "outputs": [],
   "source": [
    "enron_df['email_text'] = enron_df['Subject'] + ' ' + enron_df['email_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOUcid_C3O1N",
    "outputId": "3fd6d3de-0eda-4ffd-83fa-42f46eeeb61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed attachments\n",
      "parsed contacts\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def remove_attachment_text(text):\n",
    "    # Split the text at the specific phrase\n",
    "    parts = text.split(\"Content-Disposition: attachment;\")\n",
    "    # Return the part before the phrase if it exists\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def parse_contacts(data):\n",
    "    # count the number of '~' in the data to check if its notes\n",
    "    count = data.count('~')\n",
    "    if count < 30:\n",
    "        return data\n",
    "\n",
    "    # Normalize the data by removing line continuation characters\n",
    "    data = data.replace(\"=\\n\", \"\")  # Assumes `=` at the end of the line followed by a newline\n",
    "\n",
    "    # Split the data into individual records on '#'\n",
    "    records = data.split('#')\n",
    "\n",
    "    # Initialize a list to store parsed contacts\n",
    "    contacts = []\n",
    "\n",
    "    # Iterate through each record\n",
    "    for record in records:\n",
    "        # Split the record into fields using '~'\n",
    "        fields = record.split('~')\n",
    "\n",
    "        # TODO: need to check relevant fields\n",
    "        if len(fields) > 21:  # Check to ensure it's a valid record\n",
    "            contact = {\n",
    "                'first_name': fields[1].strip(),\n",
    "                'last_name': fields[3].strip(),\n",
    "                'phone_numbers': fields[11:14],\n",
    "                'position': fields[15].strip(),\n",
    "                'company': fields[18].strip(),\n",
    "                'email': fields[21].strip() if len(fields) > 21 else None  # Safeguard for missing email\n",
    "            }\n",
    "            contacts.append(contact)\n",
    "            # print('contact:', contact)\n",
    "\n",
    "    if not contacts:\n",
    "        return ' '\n",
    "    return json.dumps(contacts)\n",
    "\n",
    "\n",
    "enron_df['email_text'] = enron_df['email_text'].apply(remove_attachment_text)\n",
    "print('removed attachments')\n",
    "\n",
    "enron_df['email_text'] = enron_df['email_text'].apply(parse_contacts)\n",
    "enron_df['email_text'].fillna(' ', inplace=True)\n",
    "print('parsed contacts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn212ATf3O1O",
    "outputId": "c60eb8e0-03d8-40eb-f6c6-cf4318265282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with length of tokens 510 and less: 192268\n",
      "Data with length of tokens 511 and more: 325133\n"
     ]
    }
   ],
   "source": [
    "# split all data of column email_text with length of 510 and less, or 511 or more\n",
    "SPLIT_SIZE = 510\n",
    "\n",
    "def get_size(text):\n",
    "    return len(text.split())\n",
    "\n",
    "enron_df['text_size'] = enron_df['email_text'].apply(len)\n",
    "\n",
    "\n",
    "# split data with length of 510 and less on column text_size\n",
    "enron_df_small = enron_df[enron_df['text_size'] <= SPLIT_SIZE]\n",
    "enron_df_large = enron_df[enron_df['text_size'] > SPLIT_SIZE]\n",
    "\n",
    "enron_df.drop(columns=['text_size'], inplace=True)\n",
    "\n",
    "# print the number of data that will be split\n",
    "print('Data with length of tokens 510 and less:', len(enron_df_small))\n",
    "print('Data with length of tokens 511 and more:', len(enron_df_large))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDmRUJEx3O1P",
    "outputId": "df00d738-22af-4870-c304-cbf0cdb19e7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Efraim\n",
      "[nltk_data]     Yosofov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Efraim\n",
      "[nltk_data]     Yosofov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pandas import isnull\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and remove punctuation\n",
    "    if text is None or isnull(text):\n",
    "        return ''\n",
    "    text = text.replace('-', '')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stopped_tokens = [i for i in tokens if not i in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "\n",
    "    # Join the words back into one string separated by space\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "yWncVbnt3O1P"
   },
   "outputs": [],
   "source": [
    "def process_in_batches(dataset, batch_size, preprocess_text):\n",
    "    # Number of rows in the dataset\n",
    "    total_rows = len(dataset)\n",
    "\n",
    "    # Process each batch\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        # Apply the preprocess function to the batch\n",
    "        dataset.loc[start:end, 'email_text'] = dataset['email_text'][start:end].apply(preprocess_text)\n",
    "        print(f'Processed rows {start} to {end}')\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fG_xs-eA3O1Q",
    "outputId": "2778cf8e-88a6-4137-b2bd-516d4757c9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: ./bert_df_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to check if the dataset file exists to avoid recomputation\n",
    "def load_or_process_dataset(file_path):\n",
    "    try:\n",
    "        # Attempt to load the dataset from the specified file path\n",
    "        dataset = pd.read_pickle(file_path)\n",
    "        print(\"Loaded dataset from:\", file_path)\n",
    "    except (FileNotFoundError, IOError):\n",
    "        print(\"File not found. Processing dataset...\")\n",
    "        # If the file does not exist, process the data as per the original steps\n",
    "        dataset = enron_df_small[['email_text', 'violated_rules']]\n",
    "        process_in_batches(dataset, 1000, preprocess_text)\n",
    "\n",
    "        # Save the processed dataset to the specified file path\n",
    "        dataset.to_pickle(file_path)\n",
    "        print(\"Dataset processed and saved to:\", file_path)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "if IN_COLAB:\n",
    "  file_path = '/content/drive/MyDrive/Study/Cyber_AI/bert_df_processed.pkl'\n",
    "else:\n",
    "  file_path = './bert_df_processed.pkl'\n",
    "bert_dataset = load_or_process_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "XoEf9uY63O1Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Number of emails violating rules 1.2 and 2.1: 1671\n",
    "# Number of emails violating rules 1.3 and 2.2: 2094\n",
    "# Number of emails violating rules 1.3, 2.1 and 2.2: 64\n",
    "# =====================\n",
    "# Number of emails violating rules 1.1 and 1.2: 68\n",
    "# Number of emails violating rules 1.1 and 1.3: 6\n",
    "# Number of emails violating rules 1.2 and 1.3: 967\n",
    "# =====================\n",
    "# Number of emails violating rules 1.1 and 2.3: 4\n",
    "# Number of emails violating rules 1.2 and 2.3: 1176\n",
    "# Number of emails violating rules 1.3 and 2.3: 9024\n",
    "\n",
    "# create labels per rule violation, where -1 is no violation, and multiple violations are combined to new label\n",
    "def create_label(text):\n",
    "    \"\"\"\n",
    "    create labels for 1.1, 1.2, 1.3, 1.2 & 2.1, 1.3 & 2.2, 1.2 & 1.3, 1.2 & 2.3, 1.3 & 2.3. else -1\n",
    "    \"\"\"\n",
    "    if '1.1' in text:\n",
    "        return 0\n",
    "    if '1.2' in text:\n",
    "        if '2.1' in text:\n",
    "            return 1\n",
    "        if '2.3' in text:\n",
    "            return 2\n",
    "        return 3\n",
    "    if '1.3' in text:\n",
    "        if '2.2' in text:\n",
    "            return 4\n",
    "        if '2.3' in text:\n",
    "            return 5\n",
    "        return 6\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "# Load and prepare your dataset\n",
    "# dataset = pd.read_csv('your_dataset.csv')  # Assuming data is loaded from a CSV file\n",
    "bert_dataset['label'] = bert_dataset['violated_rules'].apply(create_label)\n",
    "\n",
    "# drop lines where its empty or null\n",
    "bert_dataset = bert_dataset[bert_dataset['email_text'].notna() & bert_dataset['email_text'].ne('')]\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    bert_dataset['email_text'], bert_dataset['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of email_text in enron_df_large: 2775.346821147038\n",
      "length of df large: 325133\n"
     ]
    }
   ],
   "source": [
    "# get mean length of email_text in enron_df_large \n",
    "mean_length = enron_df_large['email_text'].apply(len).mean()\n",
    "print('Mean length of email_text in enron_df_large:', mean_length)\n",
    "print('length of df large:', len(enron_df_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "29e9b75e4b0b4dad8857ab3f2d256cc5",
      "99e3607903d74a47b3268d41d62fccc9",
      "8866a8a9581b44bd972a23843e76fb4f",
      "92b95811929542609a72011c9d903664",
      "963eacf309ea49519c34ab19764b1e25",
      "7efd864cb5924cbb9e7aa7bb5621fb5a",
      "3d4f795159fe42a78d6c318149b3faf1",
      "9357c8751f1041ecbac54491f8f7446b",
      "8f9f1ff3fe1342119038627ae5da065a",
      "22b55adc5c2a42b88a0ea8993549a5be",
      "676122529f2d44509e4bd639e7ab80e3"
     ]
    },
    "id": "-KJZsh2c3O1R",
    "outputId": "70ce687f-b0ca-454c-e43e-6fc7a8c26e51"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[78], line 38\u001B[0m\n\u001B[0;32m     36\u001B[0m   topic_model \u001B[38;5;241m=\u001B[39m BERTopic\u001B[38;5;241m.\u001B[39mload(model_path)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m   topic_model \u001B[38;5;241m=\u001B[39m \u001B[43mBERTopic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mall-MiniLM-L6-v2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loaded.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\_bertopic.py:3145\u001B[0m, in \u001B[0;36mBERTopic.load\u001B[1;34m(cls, path, embedding_model)\u001B[0m\n\u001B[0;32m   3143\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_or_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m   3144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m embedding_model:\n\u001B[1;32m-> 3145\u001B[0m         topic_model \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3146\u001B[0m         topic_model\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m select_backend(embedding_model)\n\u001B[0;32m   3147\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\numpy_pickle.py:648\u001B[0m, in \u001B[0;36mload\u001B[1;34m(filename, mmap_mode)\u001B[0m\n\u001B[0;32m    646\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    647\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _read_fileobject(fobj, filename, mmap_mode) \u001B[38;5;28;01mas\u001B[39;00m fobj:\n\u001B[1;32m--> 648\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43m_unpickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    650\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\numpy_pickle.py:577\u001B[0m, in \u001B[0;36m_unpickle\u001B[1;34m(fobj, filename, mmap_mode)\u001B[0m\n\u001B[0;32m    575\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 577\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m unpickler\u001B[38;5;241m.\u001B[39mcompat_mode:\n\u001B[0;32m    579\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been generated with a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    580\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoblib version less than 0.10. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    581\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease regenerate this pickle file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    582\u001B[0m                       \u001B[38;5;241m%\u001B[39m filename,\n\u001B[0;32m    583\u001B[0m                       \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[1;32mc:\\Program Files\\Python311\\Lib\\pickle.py:1213\u001B[0m, in \u001B[0;36m_Unpickler.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1211\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[0;32m   1212\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[1;32m-> 1213\u001B[0m         \u001B[43mdispatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[0;32m   1215\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst\u001B[38;5;241m.\u001B[39mvalue\n",
      "File \u001B[1;32mc:\\Program Files\\Python311\\Lib\\pickle.py:1590\u001B[0m, in \u001B[0;36m_Unpickler.load_reduce\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1588\u001B[0m args \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39mpop()\n\u001B[0;32m   1589\u001B[0m func \u001B[38;5;241m=\u001B[39m stack[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m-> 1590\u001B[0m stack[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\storage.py:371\u001B[0m, in \u001B[0;36m_load_from_bytes\u001B[1;34m(b)\u001B[0m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_from_bytes\u001B[39m(b):\n\u001B[1;32m--> 371\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBytesIO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1040\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1039\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(UNSAFE_MESSAGE \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_legacy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopened_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1268\u001B[0m, in \u001B[0;36m_legacy_load\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1266\u001B[0m unpickler \u001B[38;5;241m=\u001B[39m UnpicklerWrapper(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m   1267\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[1;32m-> 1268\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1270\u001B[0m deserialized_storage_keys \u001B[38;5;241m=\u001B[39m pickle_module\u001B[38;5;241m.\u001B[39mload(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m   1272\u001B[0m offset \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;28;01mif\u001B[39;00m f_should_read_directly \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1205\u001B[0m, in \u001B[0;36m_legacy_load.<locals>.persistent_load\u001B[1;34m(saved_id)\u001B[0m\n\u001B[0;32m   1201\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_torch_load_uninitialized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1202\u001B[0m     \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[0;32m   1203\u001B[0m     \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[0;32m   1204\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[1;32m-> 1205\u001B[0m         wrap_storage\u001B[38;5;241m=\u001B[39m\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m   1206\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   1207\u001B[0m         _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1208\u001B[0m     deserialized_objects[root_key] \u001B[38;5;241m=\u001B[39m typed_storage\n\u001B[0;32m   1209\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:391\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[1;34m(storage, location)\u001B[0m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_restore_location\u001B[39m(storage, location):\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[1;32m--> 391\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:266\u001B[0m, in \u001B[0;36m_cuda_deserialize\u001B[1;34m(obj, location)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_cuda_deserialize\u001B[39m(obj, location):\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m location\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 266\u001B[0m         device \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate_cuda_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    267\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_torch_load_uninitialized\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    268\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:250\u001B[0m, in \u001B[0;36mvalidate_cuda_device\u001B[1;34m(location)\u001B[0m\n\u001B[0;32m    247\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_get_device_index(location, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAttempting to deserialize object on a CUDA \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    251\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice but torch.cuda.is_available() is False. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    252\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIf you are running on a CPU-only machine, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    253\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplease use torch.load with map_location=torch.device(\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    254\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto map your storages to the CPU.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    255\u001B[0m device_count \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice_count()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m device_count:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bertopic import BERTopic\n",
    "\n",
    "def train_bert_model(train_texts, train_labels):\n",
    "    # Define the path to the model\n",
    "    if IN_COLAB:\n",
    "      model_path = '/content/drive/MyDrive/Study/Cyber_AI/my_bertopic_model'\n",
    "    else:\n",
    "      model_path = './my_bertopic_model'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        # If the model does not exist, create, fit, and save a new model\n",
    "        topic_model = BERTopic(language=\"english\", verbose=True, low_memory=True)\n",
    "        topic_model.fit(train_texts, y=train_labels)\n",
    "        topic_model.save(model_path, save_embedding_model=False)\n",
    "        print(\"Model trained and saved.\")\n",
    "    else:\n",
    "        # If the model exists, load the existing model\n",
    "        if IN_COLAB:\n",
    "          topic_model = BERTopic.load(model_path)\n",
    "        else:\n",
    "          topic_model = BERTopic.load(model_path, embedding_model='all-MiniLM-L6-v2')\n",
    "        print(\"Model loaded.\")\n",
    "\n",
    "    return topic_model\n",
    "\n",
    "if 'enron_df' in globals():\n",
    "  del enron_df\n",
    "\n",
    "\n",
    "# Train the BERTopic model\n",
    "topic_model = train_bert_model(train_texts, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433,
     "referenced_widgets": [
      "b2895fecd0574d0da430b9bce8e74805",
      "6d19da861cd24aab99d243bd34823757",
      "d341b1fc4a4d4015bbfe39e02dda8619",
      "45336c5deeb046a298b61021b471e56a",
      "05d7d97dc3b941c485c797ee102bc3f5",
      "cb15afe8b545454aab7d7d27e0dbf257",
      "01559c8ae45b4fdab8957a7ee71572d2",
      "090eb23085024302ac37d40962cdb5b8",
      "e5873cc8a95c4970ae4cb3cb76df605f",
      "33941ebb60f944519959951178069607",
      "8689205ec980406dae7da2b3993be7f3"
     ]
    },
    "id": "Z6ysBWwh3O1S",
    "outputId": "355e88e2-869f-410d-d47c-dddc345cea2d"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_texts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "predicted_topics, _ = topic_model.transform(test_texts)\n",
    "\n",
    "all = zip(predicted_topics, test_labels)\n",
    "# Evaluate the Model\n",
    "# You can evaluate the model using the predicted topics and the true labels. For example, you can calculate the accuracy of the model as follows:\n",
    "accuracy = (predicted_topics == test_labels).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# check each topic if its corresponds to a label in the test_labels\n",
    "# get topic length\n",
    "topic_length = len(topic_model.get_topics())\n",
    "\n",
    "# i want each topic to see if it corresponds to a label between 0 and 6, if not, then i will check the next topic\n",
    "# each topic is row, and there is 7 column representing the labels. the value is the number of corresponding labels in the topic\n",
    "for topic_num in range(topic_length):\n",
    "    # get the topic words\n",
    "    topic_words = topic_model.get_topic(topic_num)\n",
    "    # get the topic labels\n",
    "    \n",
    "\n",
    "# Visualize the Topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure that necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text_LDA(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    return ' '.lemmatized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 records\n",
      "Processed 1000 records\n",
      "Processed 2000 records\n",
      "Processed 3000 records\n",
      "Processed 4000 records\n",
      "Processed 5000 records\n",
      "Processed 6000 records\n",
      "Processed 7000 records\n",
      "Processed 8000 records\n",
      "Processed 9000 records\n",
      "Processed 10000 records\n",
      "Processed 11000 records\n",
      "Processed 12000 records\n",
      "Processed 13000 records\n",
      "Processed 14000 records\n",
      "Processed 15000 records\n",
      "Processed 16000 records\n",
      "Processed 17000 records\n",
      "Processed 18000 records\n",
      "Processed 19000 records\n",
      "Processed 20000 records\n",
      "Processed 21000 records\n",
      "Processed 22000 records\n",
      "Processed 23000 records\n",
      "Processed 24000 records\n",
      "Processed 25000 records\n",
      "Processed 26000 records\n",
      "Processed 27000 records\n",
      "Processed 28000 records\n",
      "Processed 29000 records\n",
      "Processed 30000 records\n",
      "Processed 31000 records\n",
      "Processed 32000 records\n",
      "Processed 33000 records\n",
      "Processed 34000 records\n",
      "Processed 35000 records\n",
      "Processed 36000 records\n",
      "Processed 37000 records\n",
      "Processed 38000 records\n",
      "Processed 39000 records\n",
      "Processed 40000 records\n",
      "Processed 41000 records\n",
      "Processed 42000 records\n",
      "Processed 43000 records\n",
      "Processed 44000 records\n",
      "Processed 45000 records\n",
      "Processed 46000 records\n",
      "Processed 47000 records\n",
      "Processed 48000 records\n",
      "Processed 49000 records\n",
      "Processed 50000 records\n",
      "Processed 51000 records\n",
      "Processed 52000 records\n",
      "Processed 53000 records\n",
      "Processed 54000 records\n",
      "Processed 55000 records\n",
      "Processed 56000 records\n",
      "Processed 57000 records\n",
      "Processed 58000 records\n",
      "Processed 59000 records\n",
      "Processed 60000 records\n",
      "Processed 61000 records\n",
      "Processed 62000 records\n",
      "Processed 63000 records\n",
      "Processed 64000 records\n",
      "Processed 65000 records\n",
      "Processed 66000 records\n",
      "Processed 67000 records\n",
      "Processed 68000 records\n",
      "Processed 69000 records\n",
      "Processed 70000 records\n",
      "Processed 71000 records\n",
      "Processed 72000 records\n",
      "Processed 73000 records\n",
      "Processed 74000 records\n",
      "Processed 75000 records\n",
      "Processed 76000 records\n",
      "Processed 77000 records\n",
      "Processed 78000 records\n",
      "Processed 79000 records\n",
      "Processed 80000 records\n",
      "Processed 81000 records\n",
      "Processed 82000 records\n",
      "Processed 83000 records\n",
      "Processed 84000 records\n",
      "Processed 85000 records\n",
      "Processed 86000 records\n",
      "Processed 87000 records\n",
      "Processed 88000 records\n",
      "Processed 89000 records\n",
      "Processed 90000 records\n",
      "Processed 91000 records\n",
      "Processed 92000 records\n",
      "Processed 93000 records\n",
      "Processed 94000 records\n",
      "Processed 95000 records\n",
      "Processed 96000 records\n",
      "Processed 97000 records\n",
      "Processed 98000 records\n",
      "Processed 99000 records\n",
      "Processed 100000 records\n",
      "Processed 101000 records\n",
      "Processed 102000 records\n",
      "Processed 103000 records\n",
      "Processed 104000 records\n",
      "Processed 105000 records\n",
      "Processed 106000 records\n",
      "Processed 107000 records\n",
      "Processed 108000 records\n",
      "Processed 109000 records\n",
      "Processed 110000 records\n",
      "Processed 111000 records\n",
      "Processed 112000 records\n",
      "Processed 113000 records\n",
      "Processed 114000 records\n",
      "Processed 115000 records\n",
      "Processed 116000 records\n",
      "Processed 117000 records\n",
      "Processed 118000 records\n",
      "Processed 119000 records\n",
      "Processed 120000 records\n",
      "Processed 121000 records\n",
      "Processed 122000 records\n",
      "Processed 123000 records\n",
      "Processed 124000 records\n",
      "Processed 125000 records\n",
      "Processed 126000 records\n",
      "Processed 127000 records\n",
      "Processed 128000 records\n",
      "Processed 129000 records\n",
      "Processed 130000 records\n",
      "Processed 131000 records\n",
      "Processed 132000 records\n",
      "Processed 133000 records\n",
      "Processed 134000 records\n",
      "Processed 135000 records\n",
      "Processed 136000 records\n",
      "Processed 137000 records\n",
      "Processed 138000 records\n",
      "Processed 139000 records\n",
      "Processed 140000 records\n",
      "Processed 141000 records\n",
      "Processed 142000 records\n",
      "Processed 143000 records\n",
      "Processed 144000 records\n",
      "Processed 145000 records\n",
      "Processed 146000 records\n",
      "Processed 147000 records\n",
      "Processed 148000 records\n",
      "Processed 149000 records\n",
      "Processed 150000 records\n",
      "Processed 151000 records\n",
      "Processed 152000 records\n",
      "Processed 153000 records\n",
      "Processed 154000 records\n",
      "Processed 155000 records\n",
      "Processed 156000 records\n",
      "Processed 157000 records\n",
      "Processed 158000 records\n",
      "Processed 159000 records\n",
      "Processed 160000 records\n",
      "Processed 161000 records\n",
      "Processed 162000 records\n",
      "Processed 163000 records\n",
      "Processed 164000 records\n",
      "Processed 165000 records\n",
      "Processed 166000 records\n",
      "Processed 167000 records\n",
      "Processed 168000 records\n",
      "Processed 169000 records\n",
      "Processed 170000 records\n",
      "Processed 171000 records\n",
      "Processed 172000 records\n",
      "Processed 173000 records\n",
      "Processed 174000 records\n",
      "Processed 175000 records\n",
      "Processed 176000 records\n",
      "Processed 177000 records\n",
      "Processed 178000 records\n",
      "Processed 179000 records\n",
      "Processed 180000 records\n",
      "Processed 181000 records\n",
      "Processed 182000 records\n",
      "Processed 183000 records\n",
      "Processed 184000 records\n",
      "Processed 185000 records\n",
      "Processed 186000 records\n",
      "Processed 187000 records\n",
      "Processed 188000 records\n",
      "Processed 189000 records\n",
      "Processed 190000 records\n",
      "Processed 191000 records\n",
      "Processed 192000 records\n",
      "Processed 193000 records\n",
      "Processed 194000 records\n",
      "Processed 195000 records\n",
      "Processed 196000 records\n",
      "Processed 197000 records\n",
      "Processed 198000 records\n",
      "Processed 199000 records\n",
      "Processed 200000 records\n",
      "Processed 201000 records\n",
      "Processed 202000 records\n",
      "Processed 203000 records\n",
      "Processed 204000 records\n",
      "Processed 205000 records\n",
      "Processed 206000 records\n",
      "Processed 207000 records\n",
      "Processed 208000 records\n",
      "Processed 209000 records\n",
      "Processed 210000 records\n",
      "Processed 211000 records\n",
      "Processed 212000 records\n",
      "Processed 213000 records\n",
      "Processed 214000 records\n",
      "Processed 215000 records\n",
      "Processed 216000 records\n",
      "Processed 217000 records\n",
      "Processed 218000 records\n",
      "Processed 219000 records\n",
      "Processed 220000 records\n",
      "Processed 221000 records\n",
      "Processed 222000 records\n",
      "Processed 223000 records\n",
      "Processed 224000 records\n",
      "Processed 225000 records\n",
      "Processed 226000 records\n",
      "Processed 227000 records\n",
      "Processed 228000 records\n",
      "Processed 229000 records\n",
      "Processed 230000 records\n",
      "Processed 231000 records\n",
      "Processed 232000 records\n",
      "Processed 233000 records\n",
      "Processed 234000 records\n",
      "Processed 235000 records\n",
      "Processed 236000 records\n",
      "Processed 237000 records\n",
      "Processed 238000 records\n",
      "Processed 239000 records\n",
      "Processed 240000 records\n",
      "Processed 241000 records\n",
      "Processed 242000 records\n",
      "Processed 243000 records\n",
      "Processed 244000 records\n",
      "Processed 245000 records\n",
      "Processed 246000 records\n",
      "Processed 247000 records\n",
      "Processed 248000 records\n",
      "Processed 249000 records\n",
      "Processed 250000 records\n",
      "Processed 251000 records\n",
      "Processed 252000 records\n",
      "Processed 253000 records\n",
      "Processed 254000 records\n",
      "Processed 255000 records\n",
      "Processed 256000 records\n",
      "Processed 257000 records\n",
      "Processed 258000 records\n",
      "Processed 259000 records\n",
      "Processed 260000 records\n",
      "Processed 261000 records\n",
      "Processed 262000 records\n",
      "Processed 263000 records\n",
      "Processed 264000 records\n",
      "Processed 265000 records\n",
      "Processed 266000 records\n",
      "Processed 267000 records\n",
      "Processed 268000 records\n",
      "Processed 269000 records\n",
      "Processed 270000 records\n",
      "Processed 271000 records\n",
      "Processed 272000 records\n",
      "Processed 273000 records\n",
      "Processed 274000 records\n",
      "Processed 275000 records\n",
      "Processed 276000 records\n",
      "Processed 277000 records\n",
      "Processed 278000 records\n",
      "Processed 279000 records\n",
      "Processed 280000 records\n",
      "Processed 281000 records\n",
      "Processed 282000 records\n",
      "Processed 283000 records\n",
      "Processed 284000 records\n",
      "Processed 285000 records\n",
      "Processed 286000 records\n",
      "Processed 287000 records\n",
      "Processed 288000 records\n",
      "Processed 289000 records\n",
      "Processed 290000 records\n",
      "Processed 291000 records\n",
      "Processed 292000 records\n",
      "Processed 293000 records\n",
      "Processed 294000 records\n",
      "Processed 295000 records\n",
      "Processed 296000 records\n",
      "Processed 297000 records\n",
      "Processed 298000 records\n",
      "Processed 299000 records\n",
      "Processed 300000 records\n",
      "Processed 301000 records\n",
      "Processed 302000 records\n",
      "Processed 303000 records\n",
      "Processed 304000 records\n",
      "Processed 305000 records\n",
      "Processed 306000 records\n",
      "Processed 307000 records\n",
      "Processed 308000 records\n",
      "Processed 309000 records\n",
      "Processed 310000 records\n",
      "Processed 311000 records\n",
      "Processed 312000 records\n",
      "Processed 313000 records\n",
      "Processed 314000 records\n",
      "Processed 315000 records\n",
      "Processed 316000 records\n",
      "Processed 317000 records\n",
      "Processed 318000 records\n",
      "Processed 319000 records\n",
      "Processed 320000 records\n",
      "Processed 321000 records\n",
      "Processed 322000 records\n",
      "Processed 323000 records\n",
      "Processed 324000 records\n",
      "Processed 325000 records\n"
     ]
    }
   ],
   "source": [
    "def preprocess_wrapper(row):\n",
    "    # Access a counter that's defined outside of the function\n",
    "    global counter\n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"Processed {counter} records\")\n",
    "    result = preprocess_text_LDA(row)\n",
    "    counter += 1\n",
    "    return result\n",
    "\n",
    "counter = 0  # Initialize counter outside of apply\n",
    "enron_df_large['email_text'] = enron_df_large['email_text'].apply(preprocess_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save enron_df_large to pickle file\n",
    "if IN_COLAB:\n",
    "    enron_df_large.to_pickle('/content/drive/MyDrive/Study/Cyber_AI/enron_df_large_processed.pkl')\n",
    "else:\n",
    "    enron_df_large.to_pickle('enron_df_large_processed.pkl')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmCHEpTg3O1S"
   },
   "outputs": [],
   "source": [
    "# Build LDA model based on the enron long dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# we will work on this data\n",
    "# enron_df_large\n",
    "# Create a CountVectorizer to convert text data into a bag of words\n",
    "def train_lda_model(data, n_topics=10):\n",
    "    count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    count_data = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    # Create and fit an LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(count_data)\n",
    "\n",
    "    return lda, count_vectorizer\n",
    "\n",
    "\n",
    "# Function to display the topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LDA model\n",
    "import pickle\n",
    "\n",
    "# Save the LDA model\n",
    "with open('lda_model.pkl', 'wb') as file:\n",
    "    pickle.dump(lda, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the LDA model, or trian if not existing\n",
    "try:\n",
    "    # Attempt to load the LDA model from the specified file path\n",
    "    with open('lda_model.pkl', 'rb') as file:\n",
    "        lda = pickle.load(file)\n",
    "        print(\"Loaded LDA model from file.\")\n",
    "except (FileNotFoundError, IOError):\n",
    "    print(\"File not found. Training LDA model...\")\n",
    "    # If the file does not exist, train the LDA model\n",
    "    lda, count_vectorizer = train_lda_model(enron_df_large['email_text'])\n",
    "    print(\"LDA model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "subject know message sent like time think good pm going\n",
      "Topic 2:\n",
      "power enron said energy company state california price utility electricity\n",
      "Topic 3:\n",
      "message subject enron sent agreement pm cc thanks attached know\n",
      "Topic 4:\n",
      "company business service new market enron management group trading report\n",
      "Topic 5:\n",
      "gas price market deal contract power day rate ferc customer\n",
      "Topic 6:\n",
      "pm outage scheduled sat london ct fri pt contact data\n",
      "Topic 7:\n",
      "td font br http nbsp error tr game database updated\n",
      "Topic 8:\n",
      "ect enron cc subject pm forwarded ee enronxgate thanks mark\n",
      "Topic 9:\n",
      "http image click email free new information address offer online\n",
      "Topic 10:\n",
      "enron meeting houston time texas program way day conference travel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# display the topics, where each topic is a set of words\n",
    "no_top_words = 10\n",
    "display_topics(lda, count_vectorizer.get_feature_names_out(), no_top_words)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01559c8ae45b4fdab8957a7ee71572d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05d7d97dc3b941c485c797ee102bc3f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "090eb23085024302ac37d40962cdb5b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22b55adc5c2a42b88a0ea8993549a5be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29e9b75e4b0b4dad8857ab3f2d256cc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99e3607903d74a47b3268d41d62fccc9",
       "IPY_MODEL_8866a8a9581b44bd972a23843e76fb4f",
       "IPY_MODEL_92b95811929542609a72011c9d903664"
      ],
      "layout": "IPY_MODEL_963eacf309ea49519c34ab19764b1e25"
     }
    },
    "33941ebb60f944519959951178069607": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d4f795159fe42a78d6c318149b3faf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45336c5deeb046a298b61021b471e56a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33941ebb60f944519959951178069607",
      "placeholder": "",
      "style": "IPY_MODEL_8689205ec980406dae7da2b3993be7f3",
      "value": "757/757[00:33&lt;00:00,82.82it/s]"
     }
    },
    "676122529f2d44509e4bd639e7ab80e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d19da861cd24aab99d243bd34823757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb15afe8b545454aab7d7d27e0dbf257",
      "placeholder": "",
      "style": "IPY_MODEL_01559c8ae45b4fdab8957a7ee71572d2",
      "value": "Batches:100%"
     }
    },
    "7efd864cb5924cbb9e7aa7bb5621fb5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8689205ec980406dae7da2b3993be7f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8866a8a9581b44bd972a23843e76fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9357c8751f1041ecbac54491f8f7446b",
      "max": 3026,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f9f1ff3fe1342119038627ae5da065a",
      "value": 3026
     }
    },
    "8f9f1ff3fe1342119038627ae5da065a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92b95811929542609a72011c9d903664": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22b55adc5c2a42b88a0ea8993549a5be",
      "placeholder": "",
      "style": "IPY_MODEL_676122529f2d44509e4bd639e7ab80e3",
      "value": "3026/3026[01:56&lt;00:00,88.80it/s]"
     }
    },
    "9357c8751f1041ecbac54491f8f7446b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "963eacf309ea49519c34ab19764b1e25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99e3607903d74a47b3268d41d62fccc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7efd864cb5924cbb9e7aa7bb5621fb5a",
      "placeholder": "",
      "style": "IPY_MODEL_3d4f795159fe42a78d6c318149b3faf1",
      "value": "Batches:100%"
     }
    },
    "b2895fecd0574d0da430b9bce8e74805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d19da861cd24aab99d243bd34823757",
       "IPY_MODEL_d341b1fc4a4d4015bbfe39e02dda8619",
       "IPY_MODEL_45336c5deeb046a298b61021b471e56a"
      ],
      "layout": "IPY_MODEL_05d7d97dc3b941c485c797ee102bc3f5"
     }
    },
    "cb15afe8b545454aab7d7d27e0dbf257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d341b1fc4a4d4015bbfe39e02dda8619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_090eb23085024302ac37d40962cdb5b8",
      "max": 757,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5873cc8a95c4970ae4cb3cb76df605f",
      "value": 757
     }
    },
    "e5873cc8a95c4970ae4cb3cb76df605f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
